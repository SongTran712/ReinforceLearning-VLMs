{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109969,
     "status": "ok",
     "timestamp": 1745302976473,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "2m_Ec3B3SZVV",
    "outputId": "38b808ae-be9c-47c3-8762-4db3453b38c0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/root/.venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: datasets in /root/.venv/lib/python3.12/site-packages (3.5.0)\n",
      "Requirement already satisfied: torchvision in /root/.venv/lib/python3.12/site-packages (0.22.0)\n",
      "Requirement already satisfied: sentencepiece in /root/.venv/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in /root/.venv/lib/python3.12/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/.venv/lib/python3.12/site-packages (from datasets) (2.2.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/.venv/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /root/.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /root/.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /root/.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /root/.venv/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /root/.venv/lib/python3.12/site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /root/.venv/lib/python3.12/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /root/.venv/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: torch==2.7.0 in /root/.venv/lib/python3.12/site-packages (from torchvision) (2.7.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /root/.venv/lib/python3.12/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /root/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /root/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (79.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /root/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (1.13.3)\n",
      "Requirement already satisfied: networkx in /root/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /root/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /root/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /root/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /root/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /root/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /root/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /root/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /root/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /root/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /root/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /root/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /root/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /root/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /root/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /root/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /root/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /root/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/.venv/lib/python3.12/site-packages (from jinja2->torch==2.7.0->torchvision) (3.0.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/root/.venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/root/.venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets torchvision sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 14132,
     "status": "ok",
     "timestamp": 1745302995197,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "7veYguFjspaV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import Tuple\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7J9Pd71spaW"
   },
   "source": [
    "## Converting Image to a Sequence of Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1745303000415,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "1-lrtKNFspaX"
   },
   "outputs": [],
   "source": [
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self, img_size: int = 224, patch_size: int = 16, hidden_dim: int = 512\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # Store the input image size, the patch size and hidden dimension\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Calculate the total number of patches\n",
    "        self.num_patches = (self.img_size // self.patch_size) ** 2\n",
    "\n",
    "        # Create a convolution to extract patch embeddings\n",
    "        # in_channels=3 asummes a 3-channel image (RGB)\n",
    "        # outp_channels=hidden_dim sets the number of output channels to match the hidden dimension\n",
    "        # kernel_size=patch_size and stride=patch_size ensuring each patch is embedded separately\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=self.hidden_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        X = self.conv(X)\n",
    "\n",
    "        # Flatten the spatial dimensions (height and width) of the patch embeddings\n",
    "        # This step flattens the patch dimensions to a single dimension\n",
    "        # Output shape: (batch_size, hidden_dim, self.num_patches)\n",
    "        X = X.flatten(2)\n",
    "\n",
    "        # Transpose the dimensions to obtain the shape (batch_size, num_patches, hidden_dim)\n",
    "        # This step brings the num_patches dimension to the second position\n",
    "        # Output shape: (batch_size, self.num_patches, hidden_dim)\n",
    "        X = X.transpose(1, 2)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1745303004155,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "79pfUClUspaY",
    "outputId": "7fc1cabf-ad31-4f63-c65d-871940a9f024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image patches: torch.Size([128, 196, 512])\n"
     ]
    }
   ],
   "source": [
    "B, C, H, W = 128, 3, 224, 224  # Batch size, Channels, Height, Width\n",
    "X = torch.randn(B, C, H, W)\n",
    "\n",
    "patch_size = 16\n",
    "hidden_dim = 512\n",
    "\n",
    "patch_embeddings = PatchEmbeddings(\n",
    "    img_size=H, patch_size=patch_size, hidden_dim=hidden_dim\n",
    ")\n",
    "patches = patch_embeddings(X)\n",
    "print(f\"Shape of image patches: {patches.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1745303005332,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "NTHJZgh0spaY",
    "outputId": "3facf4a2-5bd9-4d80-e6a2-bc33eb1645b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "num_patches = (H // patch_size) ** 2\n",
    "assert patches.shape == (B, num_patches, hidden_dim), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgQvJSuFspaZ"
   },
   "source": [
    "## Attention Mechanism\n",
    "Attention Mechanism across both the vision encoder and language decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xrdt5oNPspaZ"
   },
   "source": [
    "### The implementation of the Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1745303007120,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "_Zuq1aL5spaZ"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        head_size: int,\n",
    "        dropout: float = 0.1,\n",
    "        is_decoder: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear layer for Key projection\n",
    "        self.key = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "\n",
    "        # Linear layer for Query projection\n",
    "        self.query = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "\n",
    "        # Linear layer for Value projection\n",
    "        self.value = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "\n",
    "        # Dropout layer for regularization to prevent overfitting\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Flag indicating wheter the head is used as a decoder\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Get batch size (B), sequence length (T), and embedding dimension (C) from the input tensor\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Compute Key, Query, and Value projections\n",
    "        k = self.key(x)  # Shape: (B, T, head_size)\n",
    "        q = self.query(x)  # Shape: (B, T, head_size)\n",
    "        v = self.value(x)  # SHape: (B, T, head_size)\n",
    "        wei = q @ k.transpose(-2, -1) * (C**-0.5)  # Shape: (B, T, T)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            tril = torch.tril(torch.ones(T, T, dtype=torch.bool, device=x.device))\n",
    "            wei = wei.masked_fill(mask=tril == 0, value=float(\"-inf\"))\n",
    "\n",
    "        wei = F.softmax(input=wei, dim=-1)  # Shape: (B, T, T)\n",
    "\n",
    "        # Apply Dropout to the attention probabilities for regularization\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Perform a weighted aggregation of values using the attention probabilities\n",
    "        out = wei @ v  # Shape: (B, T, head_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1745303013537,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "JzOfIfuwspaa",
    "outputId": "3a0facc5-d1a5-4656-ccd4-aa1cc436b3c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([128, 196, 16])\n"
     ]
    }
   ],
   "source": [
    "B, T, C = patches.shape  # Batch size, Sequence length, Embedding dimension\n",
    "head_size = 16  # Size of the attention head\n",
    "\n",
    "head = Head(n_embed=C, head_size=head_size)\n",
    "output = head(patches)\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1745303015496,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "eVTRkWHqspaa",
    "outputId": "3dbf5216-edc7-43d9-e45a-b490b21b0192"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, head_size), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmXF3DIhspaa"
   },
   "source": [
    "### The implementation of Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1745303017117,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "inh7qmxnspab"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.1,\n",
    "        is_decoder: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensure that the embedding dimension is divisible by the number of heads\n",
    "        assert n_embed % num_heads == 0, \"n_embed must be divisible by num_heads!\"\n",
    "\n",
    "        # Create a ModuleList of attention heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            modules=[\n",
    "                Head(\n",
    "                    n_embed=n_embed,\n",
    "                    head_size=n_embed // num_heads,\n",
    "                    dropout=dropout,\n",
    "                    is_decoder=is_decoder,\n",
    "                )\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Linear layer for projecting the concatenated head outputs\n",
    "        self.proj = nn.Linear(in_features=n_embed, out_features=n_embed)\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply each attention head to the input tensor\n",
    "        head_outputs = [\n",
    "            h(x) for h in self.heads\n",
    "        ]  # Shape: num_heads * (B, T, head_size)\n",
    "\n",
    "        # Concatenate the outputs from all heads along the last dimension\n",
    "        out = torch.cat(tensors=head_outputs, dim=-1)  # Shape: (B, T, m_embed)\n",
    "\n",
    "        # Apply the projection layer to the concatenated outputs\n",
    "        out = self.proj(out)  # Shape: (B, T, m_embed)\n",
    "\n",
    "        # Apply Dropout to the projected outputs for regularization\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1745303018449,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "4kRG1mj7spab"
   },
   "outputs": [],
   "source": [
    "num_heads = 2\n",
    "dropout = 0.1\n",
    "mha = MultiHeadAttention(n_embed=C, num_heads=num_heads, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1745303019397,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "7oaAkaZ7spab",
    "outputId": "edcb66de-0bac-48a3-a5dd-7fa3f498577d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([128, 196, 512])\n"
     ]
    }
   ],
   "source": [
    "output = mha(patches)\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1745303020699,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "0I5qysIxspab",
    "outputId": "ae2793fc-fdca-424d-ae91-5d23e4855d6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, C), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ona5tht3spac"
   },
   "source": [
    "### The Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745303022869,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "1QPxO9cWspac"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_embed: int, dropout: float = 0.1, is_decoder: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the layers of the MLP\n",
    "        layers = [\n",
    "            # First linear layer that expands the input dimension from n_embed to 4 * n_embed\n",
    "            nn.Linear(in_features=n_embed, out_features=4 * n_embed),\n",
    "            # Activation function: ReLU if is_decoder is True, else GELU\n",
    "            nn.ReLU() if is_decoder else nn.GELU(),\n",
    "            # Second linear layer that projects the intermediate dimension back to n_embed\n",
    "            nn.Linear(in_features=4 * n_embed, out_features=n_embed),\n",
    "            # Dropout layer for regularization\n",
    "            nn.Dropout(p=dropout),\n",
    "        ]\n",
    "\n",
    "        # Create the MLP as a sequence of layers\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Pass the input through the MLP layers\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1745303024553,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "BK7FPIPyspac"
   },
   "outputs": [],
   "source": [
    "dropout = 0.1\n",
    "mlp = MLP(n_embed=C, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1745303025568,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "AF3WnIsospac",
    "outputId": "312131d6-dd21-4e99-8d6f-2562502a136a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([128, 196, 512])\n"
     ]
    }
   ],
   "source": [
    "output = mlp(output)  # Previous output of the Multihead Attention\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1745303026688,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "0Gj0NrF8spad",
    "outputId": "31f9ad8f-1f94-4d50-aed2-8cce238d2d72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, C), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6888PMDpspad"
   },
   "source": [
    "### Transformer Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1745303028511,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "jLy1uQa2spad"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.1,\n",
    "        is_decoder: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Layer normalization for the input to the attention layer\n",
    "        self.ln1 = nn.LayerNorm(normalized_shape=n_embed)\n",
    "\n",
    "        # Multi-head attention module\n",
    "        self.mhattn = MultiHeadAttention(\n",
    "            n_embed=n_embed, num_heads=num_heads, dropout=dropout, is_decoder=is_decoder\n",
    "        )\n",
    "\n",
    "        # Layer normalization for the input to the FFN\n",
    "        self.ln2 = nn.LayerNorm(normalized_shape=n_embed)\n",
    "\n",
    "        # Feed-forward neural network (FFN)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_features=n_embed, out_features=4 * n_embed),\n",
    "            nn.GELU(),  # Activation function\n",
    "            nn.Linear(\n",
    "                in_features=4 * n_embed, out_features=n_embed\n",
    "            ),  # Projection back to the original dimension\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Saving the input for residual connection\n",
    "        original_x = x\n",
    "\n",
    "        # Apply layer normalization to the input\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        # Apply multi-head attention\n",
    "        mhattn_output = self.mhattn(x)\n",
    "\n",
    "        # Add the residual connection (original input) to the attention output\n",
    "        x = original_x + mhattn_output\n",
    "\n",
    "        # Apply later normalization to the input to the FFN\n",
    "        x = self.ln2(x)\n",
    "\n",
    "        # Apply the FFN\n",
    "        ffn_output = self.ffn(x)\n",
    "\n",
    "        # Apply the residual connection (input to the FFN) to the FFN output\n",
    "        x = x + ffn_output\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1745303029987,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "trl5mtK5spad"
   },
   "outputs": [],
   "source": [
    "num_heads = 2\n",
    "dropout = 0.1\n",
    "block = Block(n_embed=C, num_heads=num_heads, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745303031188,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "xZMaSLSCspad",
    "outputId": "072fba58-82ed-4adb-dc7d-0eea71693846"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([128, 196, 512])\n"
     ]
    }
   ],
   "source": [
    "output = block(patches)\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1745303032249,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "POugdyswspad",
    "outputId": "c467b86a-b268-4fdb-b1ac-08cd975e6327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, C), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h31vzsbQspae"
   },
   "source": [
    "## Vision Encoder - Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DNEtMj-spae"
   },
   "source": [
    "Combining patchification logic and attention block in to ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745303034064,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "YGX1s-qCspae"
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int,\n",
    "        patch_size: int,\n",
    "        num_hiddens: int,\n",
    "        num_heads: int,\n",
    "        num_blocks: int,\n",
    "        emb_dropout: float,\n",
    "        block_dropout: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding layer to convert the input image into patches\n",
    "        self.patch_embedding = PatchEmbeddings(\n",
    "            img_size=img_size, patch_size=patch_size, hidden_dim=num_hiddens\n",
    "        )\n",
    "\n",
    "        # Learnable classification token\n",
    "        self.cls_token = nn.Parameter(data=torch.zeros(size=(1, 1, num_hiddens)))\n",
    "\n",
    "        # Calculate the number of patches\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Learnable position embedding\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            data=torch.randn(size=(1, num_patches + 1, num_hiddens))\n",
    "        )\n",
    "\n",
    "        # Dropout layer for the embeddings\n",
    "        self.dropout = nn.Dropout(p=emb_dropout)\n",
    "\n",
    "        # Stack of transformer blocks\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    n_embed=num_hiddens,\n",
    "                    num_heads=num_heads,\n",
    "                    dropout=block_dropout,\n",
    "                    is_decoder=False,\n",
    "                )\n",
    "                for _ in range(num_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Layer normalization for the final representation\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=num_hiddens)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        # Convert the input image into patch embeddings\n",
    "        x = self.patch_embedding(X)  # Shape: (B, num_patches, num_hiddens)\n",
    "\n",
    "        # Expand the classification token to match the batch size\n",
    "        cls_tokens = self.cls_token.expand(\n",
    "            x.shape[0], -1, -1\n",
    "        )  # Shape: (B, 1, num_hiddens)\n",
    "\n",
    "        # Concatenate the classification token with the patch embeddings\n",
    "        x = torch.cat(\n",
    "            tensors=(cls_tokens, x), dim=1\n",
    "        )  # Shape: (B, num_patches + 1, num_hiddens)\n",
    "\n",
    "        # Add the position embedding to the patch embeddings\n",
    "        x += self.pos_embedding  # Shape: (B, num_patches + 1, num_hiddens)\n",
    "\n",
    "        # Apply dropout to the embeddings\n",
    "        x = self.dropout(x)  # Shape: (B, num_patches + 1, num_hiddens)\n",
    "\n",
    "        # Pass the embeddings through the transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)  # Shape: (B, num_patches + 1, num_hiddens)\n",
    "\n",
    "        # Apply layer normalization to the `[CLS]` token's final representation\n",
    "        x = self.layer_norm(x[:, 0])  # Shape: (B, num_hiddens)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745303036549,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "cYwJwvrjspae"
   },
   "outputs": [],
   "source": [
    "B, C, H, W = 2, 3, 224, 224  # Batch size, Channels, Height, Width\n",
    "XX = torch.randn(B, C, H, W)\n",
    "vit = ViT(\n",
    "    img_size=H,\n",
    "    patch_size=16,\n",
    "    num_hiddens=64,\n",
    "    num_heads=2,\n",
    "    num_blocks=2,\n",
    "    emb_dropout=0.1,\n",
    "    block_dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1745303037969,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "GVwEBk8Sspae",
    "outputId": "0288437a-38f5-461c-df7d-d82273687366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "output_image = vit(XX)\n",
    "print(f\"Output shape: {output_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1745303039213,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "OCrJkhG2spaf",
    "outputId": "b48d4ff4-e87d-4368-d86b-ccdd8ab5e2f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output_image.shape == (B, 64), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1teH1MPspaf"
   },
   "source": [
    "## Vision-Language Projection Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EuAJV2rspaf"
   },
   "source": [
    "Unfortunatelly, we can't directly concatenate ViT output to the text embeddings. <br>\n",
    "We need to project this from dimensionality of image embeddings from the vision transformer to the dimensionality of text embeddings.\n",
    "\n",
    "Why MLP for this part? If you want to train VLM with low resources you can do so by keeping both the pretrained vision encoder and language decoder frozen during the VLM training. Therefore, allocating more parameters to the connection module could enhance the overall VLM's ability to generalize and help in the downstream instruction-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1745303043943,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "fsAv9P6aspaf"
   },
   "outputs": [],
   "source": [
    "class MultiModalProjector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        img_embed_dim: int,\n",
    "        dropout: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the projection network\n",
    "        self.net = nn.Sequential(\n",
    "            # Linear layer to expand the image embedding dimension\n",
    "            nn.Linear(in_features=img_embed_dim, out_features=4 * img_embed_dim),\n",
    "            # GELU activation function\n",
    "            nn.GELU(),\n",
    "            # Linear layer to project the expanded image embeddings to the text embedding dimension\n",
    "            nn.Linear(in_features=4 * img_embed_dim, out_features=n_embed),\n",
    "            # Dropout layer for regularization\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Pass the input through the projection network\n",
    "        x = self.net(x)  # Shape: (B, img_embed_dim) --> (B, n_embed)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1745303045845,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "l7ZYNX_sspaf"
   },
   "outputs": [],
   "source": [
    "B, n_embed, img_embed_dim = 2, 64, 128\n",
    "X = torch.randn(size=(B, img_embed_dim))\n",
    "\n",
    "projector = MultiModalProjector(\n",
    "    n_embed=n_embed, img_embed_dim=img_embed_dim, dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745303046992,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "TR2UZ-Kgspaf",
    "outputId": "387d1e35-1629-4f3c-91ab-ed7485ea339c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "output = projector(X)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1745303048116,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "TYJa4bTqspaf",
    "outputId": "772a624c-8048-4b2a-d870-13f27f2ef4a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, n_embed), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYlkygxcspag"
   },
   "source": [
    "## Building the Decoder Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unGnK01Yspag"
   },
   "source": [
    "Only thing that deviates from origianl implementation is that here projection module is integrated into decoder model class. <br>\n",
    "In contrary, when using pretrained models with HuggingFace (or any other library), you can directly feed embeddings as input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745303049998,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "UMcEL2qvspag"
   },
   "outputs": [],
   "source": [
    "class DecoderLanguageModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        img_embed_dim: int,\n",
    "        vocab_size: int,\n",
    "        num_heads: int,\n",
    "        n_layer: int,\n",
    "        num_labels: int,\n",
    "        \n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # self.use_images = use_images\n",
    "\n",
    "        # Token embedding table\n",
    "        self.token_embedding_table = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=n_embed\n",
    "        )\n",
    "\n",
    "        # Position embedding table\n",
    "        self.position_embedding_table = nn.Embedding(\n",
    "            num_embeddings=1000, embedding_dim=n_embed\n",
    "        )\n",
    "\n",
    "            # Image projection layer to align image embeddings with text embeddings\n",
    "        self.image_projection = MultiModalProjector(\n",
    "            n_embed=n_embed, img_embed_dim=img_embed_dim\n",
    "        )\n",
    "\n",
    "        # Stack of transformer decoder blocks\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                Block(n_embed=n_embed, num_heads=num_heads, is_decoder=True)\n",
    "                for _ in range(n_layer)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # # Final layer normalization\n",
    "        # self.ln_f = nn.LayerNorm(normalized_shape=n_embed)\n",
    "        \n",
    "        # # Language modeling head\n",
    "        # self.lm_head = nn.Linear(in_features=n_embed, out_features=num_labels)\n",
    "        \n",
    "        # self.pool =  # Global averaging\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        # Output layer: input is flattened (seq_len * embed)\n",
    "        self.lm_head = nn.Linear(n_embed*257, num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        idx: torch.Tensor,\n",
    "        img_embeds: torch.Tensor = None,\n",
    "        targets: torch.Tensor = None,\n",
    "        use_images: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        # Get token embeddings from the input indices\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "\n",
    "        if use_images:\n",
    "            # Project and concatenate image embeddings with token embeddings\n",
    "            img_emb = self.image_projection(img_embeds).unsqueeze(1)\n",
    "            tok_emb = torch.cat([img_emb, tok_emb], dim=1)\n",
    "\n",
    "        # Get position embeddings\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            torch.arange(tok_emb.size(1), device=idx.device)\n",
    "        )\n",
    "\n",
    "        # Add position embeddings to token embeddings\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # Pass through the transformer decoder blocks\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # Apply final layer normalization\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        \n",
    "        x = x.flatten(start_dim=1)\n",
    "        logits = self.lm_head(x)    # (batch, num_labels)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        if targets is not None:\n",
    "        #     if use_images and img_embeds is not None:\n",
    "        #         # Prepare targets by concatenating a dummy target for the image embedding\n",
    "        #         batch_size = idx.size(0)\n",
    "        #         targets = torch.cat(\n",
    "        #             [\n",
    "        #                 torch.full(\n",
    "        #                     (batch_size, 1), -100, dtype=torch.long, device=idx.device\n",
    "        #                 ),\n",
    "        #                 targets,\n",
    "        #             ],\n",
    "        #             dim=1,\n",
    "        #         )\n",
    "            # Compute the cross-entropy loss\n",
    "            loss = F.cross_entropy(\n",
    "                input=probs,\n",
    "                target=targets,\n",
    "                # ignore_index=-100,\n",
    "            )\n",
    "            return probs, loss\n",
    "\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB_uIrRsspag"
   },
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1786,
     "status": "ok",
     "timestamp": 1745303059188,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "dOeSBdLCspag",
    "outputId": "438d6160-7564-4c88-f3dc-b2a576240036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([10, 4]), Loss: 1.3475735187530518\n",
      "tensor([[0.2271, 0.1391, 0.1618, 0.4719],\n",
      "        [0.1752, 0.2080, 0.1337, 0.4831],\n",
      "        [0.2805, 0.2453, 0.1593, 0.3149],\n",
      "        [0.2415, 0.4050, 0.1362, 0.2173],\n",
      "        [0.1907, 0.2563, 0.1449, 0.4080],\n",
      "        [0.3991, 0.1554, 0.1956, 0.2499],\n",
      "        [0.1089, 0.3734, 0.1058, 0.4119],\n",
      "        [0.2338, 0.2527, 0.1154, 0.3981],\n",
      "        [0.4718, 0.2302, 0.1137, 0.1843],\n",
      "        [0.3386, 0.1819, 0.1926, 0.2868]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n_embed, img_embed_dim, vocab_size, num_heads, n_layer = 128, 256, 1000, 8, 6\n",
    "# `n_layer` is used to represent number of decoder transformer blocks and num_blocks for the vision encoder to avoid confusion\n",
    "device = 'cpu'\n",
    "model = DecoderLanguageModel(\n",
    "    n_embed=n_embed,\n",
    "    img_embed_dim=img_embed_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    num_heads=num_heads,\n",
    "    n_layer=n_layer,\n",
    "    # use_images=True,\n",
    "    num_labels= 4\n",
    ")\n",
    "\n",
    "\n",
    "# Dummy input\n",
    "B, T = 10, 256\n",
    "idx = torch.randint(low=0, high=vocab_size, size=(B, T)).to(device)\n",
    "image_embeds = torch.randn(B, 256).to(device)  # Assume img_embed_dim is 256\n",
    "\n",
    "targets = torch.randint(0, 4, (B,)).to(device)\n",
    "\n",
    "# targets = None\n",
    "# Test forward pass\n",
    "# Check if you need to calculate loss by providing targets\n",
    "if targets is not None:\n",
    "    logits, loss = model(idx, image_embeds, targets, True)\n",
    "    print(f\"Logits shape: {logits.shape}, Loss: {loss}\")\n",
    "    print(logits)\n",
    "else:\n",
    "    logits = model(idx, image_embeds, True)  # Call without targets\n",
    "    print(f\"Logits shape: {logits.shape}\")\n",
    "    print(logits)\n",
    "\n",
    "# # Test generation\n",
    "# generated = model.generate(idx, image_embeds, max_new_tokens=20)\n",
    "# print(f\"Generated sequence shape: {generated.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93X7Cpv3spah"
   },
   "source": [
    "## Putting everything together: Simple Vision Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1745303062504,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "629X82iGspah"
   },
   "outputs": [],
   "source": [
    "class VisionLanguageModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        img_embed_dim: int,\n",
    "        vocab_size: int,\n",
    "        n_layer: int,\n",
    "        img_size: int,\n",
    "        patch_size: int,\n",
    "        num_heads: int,\n",
    "        num_blocks: int,\n",
    "        emb_dropout: float,\n",
    "        block_dropout: float,\n",
    "        num_labels : int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Set num_hiddens equal to img_embed_dim\n",
    "        num_hiddens = img_embed_dim\n",
    "\n",
    "        # Assert that num_hiddens is divisible by num_heads\n",
    "        assert num_hiddens % num_heads == 0, ValueError(\n",
    "            \"num_hiddens must be divisible by num_heads!\"\n",
    "        )\n",
    "\n",
    "        # Initialize the Vision Transformer (ViT) encoder\n",
    "        self.vision_encoder = ViT(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            num_hiddens=num_hiddens,\n",
    "            num_heads=num_heads,\n",
    "            num_blocks=num_blocks,\n",
    "            emb_dropout=emb_dropout,\n",
    "            block_dropout=block_dropout,\n",
    "        )\n",
    "\n",
    "        # Initialize the Language Model Decoder (DecoderLanguageModel)\n",
    "        self.decoder = DecoderLanguageModel(\n",
    "            n_embed=n_embed,\n",
    "            img_embed_dim=img_embed_dim,\n",
    "            vocab_size=vocab_size,\n",
    "            num_heads=num_heads,\n",
    "            n_layer=n_layer,\n",
    "            # use_images=True,\n",
    "            num_labels= num_labels\n",
    "        )\n",
    "\n",
    "    def _check_image_embeddings(self, image_embeds: torch.Tensor) -> None:\n",
    "        \"\"\"Chek if image embeddings are valid.\"\"\"\n",
    "        if image_embeds.nelement() == 0 or image_embeds.shape[1] == 0:\n",
    "            raise ValueError(\n",
    "                \"Something is wrong with the ViT model. It's returning an empty tensor or the embedding dimension is empty.\"\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self, img_array: torch.Tensor, idx: torch.Tensor, targets: torch.Tensor = None, use_images: bool = False\n",
    "    ) -> torch.Tensor | Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Get the image embeddings from the Vision Encoder\n",
    "        image_embeds = self.vision_encoder(img_array)\n",
    "\n",
    "        # Check if image embeddings are valid\n",
    "        self._check_image_embeddings(image_embeds)\n",
    "\n",
    "        if targets is not None:\n",
    "            # If targets are provided, compute the logits and loss\n",
    "            logits, loss = self.decoder(idx, image_embeds, targets, use_images = use_images)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            # If targets are not provided, compute only the logits\n",
    "            logits = self.decoder(idx, image_embeds, use_images = use_images)\n",
    "            return logits\n",
    "\n",
    "    # def generate(\n",
    "    #     self, img_array: torch.Tensor, idx: torch.Tensor, max_new_tokens: int\n",
    "    # ) -> torch.Tensor:\n",
    "    #     # Get the image embeddings from the Vision Encoder\n",
    "    #     image_embeds = self.vision_encoder(img_array)\n",
    "\n",
    "    #     # Check if image embeddings are valid\n",
    "    #     self._check_image_embeddings(image_embeds)\n",
    "\n",
    "    #     # Generate new tokens using the Language Model Decoder\n",
    "    #     generated_tokens = self.decoder.generate(\n",
    "    #         idx=idx, img_embeds=image_embeds, max_new_tokens=max_new_tokens\n",
    "    #     )\n",
    "    #     return generated_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oK3RP2Sspai"
   },
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 520,
     "status": "ok",
     "timestamp": 1745303065738,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "olxCWjIbspai",
    "outputId": "670ca330-1ee2-49be-89f3-5abe42e7516c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0905, 0.2780, 0.2719, 0.3597],\n",
      "        [0.1561, 0.0926, 0.2483, 0.5030],\n",
      "        [0.1294, 0.3380, 0.3448, 0.1878],\n",
      "        [0.1849, 0.1907, 0.3336, 0.2908],\n",
      "        [0.0865, 0.2823, 0.1952, 0.4360],\n",
      "        [0.1445, 0.2229, 0.4012, 0.2315],\n",
      "        [0.1664, 0.2331, 0.2865, 0.3139],\n",
      "        [0.0610, 0.2035, 0.2895, 0.4460],\n",
      "        [0.2309, 0.1821, 0.2397, 0.3473],\n",
      "        [0.1294, 0.1271, 0.2939, 0.4496],\n",
      "        [0.2071, 0.2833, 0.2012, 0.3084],\n",
      "        [0.0940, 0.4006, 0.2629, 0.2424],\n",
      "        [0.1859, 0.4425, 0.0983, 0.2733],\n",
      "        [0.2073, 0.2840, 0.3508, 0.1579],\n",
      "        [0.0700, 0.4396, 0.2003, 0.2900],\n",
      "        [0.2860, 0.2504, 0.2210, 0.2425]], grad_fn=<SoftmaxBackward0>)\n",
      "Output from initialization forward pass: tensor([3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 1, 1, 2, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "n_embed, num_hiddens, vocab_size, num_heads, n_layer = 128, 512, 1000, 8, 8\n",
    "image_embed_dim = num_hiddens\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "num_blocks = 2\n",
    "device = 'cpu'\n",
    "\n",
    "n_layer, block_size, num_hiddens = 8, 32, 512\n",
    "\n",
    "model = VisionLanguageModel(\n",
    "    n_embed=n_embed,\n",
    "    img_embed_dim=image_embed_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    n_layer=n_layer,\n",
    "    img_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    num_heads=num_heads,\n",
    "    num_blocks=num_blocks,\n",
    "    emb_dropout=0.1,\n",
    "    block_dropout=0.1,\n",
    "    num_labels = 4\n",
    ")\n",
    "\n",
    "dummy_img = torch.randn(16, 3, img_size, img_size).to(\n",
    "    device\n",
    ")  \n",
    "\n",
    "dummy_idx = torch.randint(0, vocab_size, (16, 256)).to(\n",
    "    device\n",
    ")  \n",
    "\n",
    "try:\n",
    "    output = model(dummy_img, dummy_idx, use_images = True)  # Output for debugging\n",
    "    print(output)\n",
    "    predictions = torch.argmax(output, dim=-1)\n",
    "    print(\"Output from initialization forward pass:\", predictions)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Runtime Error during forward pass: {str(e)}\")\n",
    "    print(\"Check layer configurations and input shapes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745303068888,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "A5hDtsN9spai",
    "outputId": "fe07c6d1-2772-48d5-c7d2-bd880d4ecdbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ph6-Ca37z5ai"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert 1 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1745303159482,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "gIY0SvARSxKh"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_len = 256  # giới hạn token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1745303160893,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "YYllzhImS9BU"
   },
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "017b1fd478bc493c9eae5aafc36ca0ed",
      "e19b6faf003d4d458b22ae755644dd4e",
      "cc0b64d960f34603b9a191b1e55eddef",
      "50b7ca0016e74b9b86b406a58f0eb37a",
      "42a90f9929544cbb9b3540878f1ab9ff",
      "43ae542de31241efbf99b13b7b043359",
      "a6ced33ea5a04e5aae45deb1db6939df",
      "4b38adec2ad1413c8b8dabcd41a3d9ad",
      "add9d266ded843afa4819be93e994c03",
      "704f403601be451db32201221aba1b78",
      "fa7f6a094ac349f2b39e59675f1d6539"
     ]
    },
    "executionInfo": {
     "elapsed": 35730,
     "status": "ok",
     "timestamp": 1745303222813,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "-zyZX5q_S-DY",
    "outputId": "1623872e-3a22-4215-fa1b-b04312b5387e"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"HuggingFaceM4/the_cauldron\", \"ai2d\", split=\"train\")\n",
    "\n",
    "# Lọc những entry có ảnh và text hợp lệ\n",
    "dataset = dataset.filter(lambda x: x[\"images\"] and x[\"texts\"] and \"user\" in x[\"texts\"][0])\n",
    "\n",
    "# # Lấy 100 sample đầu tiên, lấy full dataset thì bỏ qua dòng này\n",
    "# dataset = dataset.select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.map(lambda x: {'image': image_transform(x['images'][0])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "print(image_transform(dataset['images'][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.PngImagePlugin.PngImageFile image mode=RGB size=299x227>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_map = ['A', 'B', 'C', 'D']\n",
    "# pad_id = tokenizer.pad_id() if tokenizer.pad_id() >= 0 else 0\n",
    "max_len = 256\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "dataset = dataset.map(lambda x: {'targets': list_map.index(re.search(r\"Answer:\\s*([A-Z])\",x['texts'][0]['assistant']).group(1)),\n",
    "                                 'input': \n",
    "                                     tokenizer(\n",
    "                                         re.sub(r'Answer with the letter\\.', 'Answer with the number only.'\n",
    "                                                           ,  re.sub(r'\\b([A-D])\\.', lambda m: f\"{ord(m.group(1)) - ord('A')}.\", x['texts'][0]['user']))\n",
    "                                                    , truncation=True, padding='max_length', max_length=max_len)['input_ids']\n",
    "                                                    ,\n",
    "                                 'image': image_transform(x['images'][0])\n",
    "                                 })\n",
    "# assert 1 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['images', 'texts', 'targets', 'input', 'image'],\n",
       "    num_rows: 2434\n",
       "})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = sum(dataset.map(lambda x: {\n",
    "#     'input': list(\n",
    "#         map(\n",
    "#             lambda t: list_map.index(re.search(r\"Answer:\\s*([A-Z])\", t['assistant']).group(1)),\n",
    "#             x['texts']\n",
    "#         )\n",
    "#     )\n",
    "# })['input'], [])\n",
    "# print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "f03af258ff3f44b198a34a4dfffb99ea",
      "c49155ad5e0744d3b840c444f87fe4a3",
      "5c46284f51b7416dacae0d9a67ee986c",
      "3df75f9d77464f19b10483b1b04468f0",
      "00dbc84d65d14669af49353af2fe81c0",
      "45d1881f95724550bf82bc6bc85d19fd",
      "97570c5dc77d4ebf92007397ddf01d64",
      "637ef79ba39b4eb18061ca44840569e4",
      "b31bca7a1ee94fb780cf90a8ab4bfb52",
      "7f2e003ba2e445658cf925566c53afe8",
      "cdc95decd0694b5396f08e64aed715ae",
      "45d13ed6fef54059ae2c0c62f2e3481d",
      "a85684c8ea1c41288d8e4a4d4b5c6b1f",
      "0f46dd8c14094ae19910136bb2ec19a6",
      "aacec83f466a43779ecc05ba9fada696",
      "e60330e7e3704f119f666acb11fb93be",
      "ee9e868961f64ab5843beb0aa6841216",
      "3d8c3f01e1034f129e1e06e17bad751f",
      "202fee786bf74b198c48307ede3c55c6",
      "2a00aa99569f4bff8d506a6a282624f1",
      "cbac3bf0c58a417996f62f2f2dd58c4b",
      "7aab307a4aa44c1a9f439089284a2a30"
     ]
    },
    "executionInfo": {
     "elapsed": 57214,
     "status": "ok",
     "timestamp": 1744964343594,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "00mLCaeIuqXb",
    "outputId": "2812104e-868f-44f2-df87-d04acb19c1f0"
   },
   "outputs": [],
   "source": [
    "# # Preprocessing function\n",
    "# def preprocess(example):\n",
    "#     img_data = example[\"images\"][0]\n",
    "\n",
    "#     # Đảm bảo ảnh là PIL.Image\n",
    "#     if isinstance(img_data, Image.Image):\n",
    "#         img = img_data.convert(\"RGB\")\n",
    "#     elif isinstance(img_data, np.ndarray):\n",
    "#         img = Image.fromarray(img_data).convert(\"RGB\")\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported image format: {type(img_data)}\")\n",
    "\n",
    "#     # Transform để ra Tensor\n",
    "#     image = image_transform(img)  # Tensor (3, 224, 224)\n",
    "\n",
    "#     # Tokenize prompt và target\n",
    "#     prompt = example[\"texts\"][0][\"user\"]\n",
    "#     target = example[\"texts\"][0].get(\"assistant\", \"\")\n",
    "\n",
    "#     # full_input = prompt + \"\\n\" + target if target else prompt\n",
    "\n",
    "#     pad_id = tokenizer.pad_id() if tokenizer.pad_id() >= 0 else 0\n",
    "#     tokens = tokenizer.encode(prompt)\n",
    "#     tokens = tokens[:max_len]\n",
    "#     tokens += [pad_id] * (max_len - len(tokens))\n",
    "#     input_ids = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "#     # Tokenize target\n",
    "#     if target:\n",
    "#         target_tokens = tokenizer.encode(target)\n",
    "#         target_tokens = target_tokens[:max_len]\n",
    "#         target_tokens += [pad_id] * (max_len - len(target_tokens))\n",
    "#         target_ids = torch.tensor(target_tokens, dtype=torch.long)\n",
    "#     else:\n",
    "#         target_ids = torch.full_like(input_ids, fill_value=pad_id)\n",
    "\n",
    "#     return {\n",
    "#         \"image\": image,\n",
    "#         \"input_ids\": input_ids,\n",
    "#         \"target_ids\": target_ids\n",
    "#     }\n",
    "\n",
    "# # Apply preprocessing\n",
    "# # print(preprocess(dataset[0]))\n",
    "# dataset = dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "dhawzqJIB7J2"
   },
   "outputs": [],
   "source": [
    "# Bắt buộc để giữ tensor thay vì list!\n",
    "dataset.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  3160,  1024,  2054,  2079, 24501, 16781,  1998, 16513,  2507,\n",
       "         2041,  9804,  1024,  1014,  1012,  7722,  1015,  1012,  6351, 14384,\n",
       "         1016,  1012, 14114,  1017,  1012,  3684,  3437,  2007,  1996,  2193,\n",
       "         2069,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['input'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1744964388474,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "KB6Kf-3u6FQs",
    "outputId": "3b4e2bdf-0e1b-47f5-9b93-d8a6438d4421"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
      "tensor([  101,  3160,  1024,  2054,  2079, 24501, 16781,  1998, 16513,  2507,\n",
      "         2041,  9804,  1024,  1014,  1012,  7722,  1015,  1012,  6351, 14384,\n",
      "         1016,  1012, 14114,  1017,  1012,  3684,  3437,  2007,  1996,  2193,\n",
      "         2069,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0])\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra xem data có đúng tensor ko\n",
    "example = dataset[0]\n",
    "print(type(example['images']))          # <class 'torch.Tensor'>\n",
    "print(example['image'])          # torch.Size([3, 224, 224])\n",
    "print(example['input'])      # torch.Size([256])\n",
    "print(example['targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "N0PmK6P63K_Y"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Đảm bảo chuyển về tensor đúng shape\n",
    "    imgs = torch.stack([torch.tensor(item['images']) if not isinstance(item['image'], torch.Tensor) else item['image'] for item in batch])\n",
    "    input_ids = torch.stack([item['input'] for item in batch])\n",
    "    target_ids = torch.stack([item['targets'] for item in batch])\n",
    "    return imgs, input_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "La0FpbzP25iX"
   },
   "outputs": [],
   "source": [
    "n_embed, num_hiddens, num_heads, n_layer = 128, 512, 8, 8\n",
    "image_embed_dim = num_hiddens\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "num_blocks = 2\n",
    "\n",
    "n_layer, block_size, num_hiddens = 8, 32, 512\n",
    "\n",
    "# Initialize the model\n",
    "vlm = VisionLanguageModel(\n",
    "    n_embed=n_embed,\n",
    "    img_embed_dim=image_embed_dim,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_layer=n_layer,\n",
    "    img_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    num_heads=num_heads,\n",
    "    num_blocks=num_blocks,\n",
    "    emb_dropout=0.1,\n",
    "    block_dropout=0.1,\n",
    "    num_labels=4\n",
    ")\n",
    "device = torch.device('cpu')\n",
    "vlm.to(device)\n",
    "\n",
    "# Optimizer, chọn bộ phù hợp, chưa thử nhiều nên không bt bộ nào tốt\n",
    "# optimizer = torch.optim.AdamW(vlm.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.SGD(vlm.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "H9ulNDAvtI9r"
   },
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46403,
     "status": "ok",
     "timestamp": 1744964478616,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "j-FwZlCz26ew",
    "outputId": "a025d600-6ecc-4d37-ce3e-731a47c125a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 305/305 [04:57<00:00,  1.03it/s, loss=1.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 1.4788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 305/305 [04:55<00:00,  1.03it/s, loss=1.74] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 1.4806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 305/305 [04:55<00:00,  1.03it/s, loss=1.24] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 1.4793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 305/305 [04:55<00:00,  1.03it/s, loss=1.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Avg Loss: 1.4793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 305/305 [04:55<00:00,  1.03it/s, loss=1.74] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Loss: 1.4806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 305/305 [04:55<00:00,  1.03it/s, loss=1.74] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Avg Loss: 1.4806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 305/305 [04:55<00:00,  1.03it/s, loss=1.74] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Avg Loss: 1.4806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 305/305 [04:55<00:00,  1.03it/s, loss=1.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Avg Loss: 1.4806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 305/305 [04:55<00:00,  1.03it/s, loss=1.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Avg Loss: 1.4806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 305/305 [04:54<00:00,  1.03it/s, loss=1.74] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Avg Loss: 1.4806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "vlm.train()\n",
    "for epoch in range(10):\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}\")\n",
    "    total_loss = 0\n",
    "    for imgs, input_ids, target_ids in pbar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        imgs = imgs.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        _, loss = vlm(imgs, input_ids, targets=target_ids, use_images = True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Avg Loss: {total_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1744964500753,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "Lr6G_VbLEm7L",
    "outputId": "8865130a-4299-4445-9989-55a6eea60061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(vlm.state_dict(), \"model_encoder.pth\")\n",
    "print(\"Model saved to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0x8bHVxvz8hB"
   },
   "source": [
    "## Eval\n",
    "### define model phải giống với lúc train, img_size=224, nếu đổi img_size phải đổi ở hàm def preprocess(example) và image_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1744964512054,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "wr6DhEADsp13",
    "outputId": "143ab83b-06f2-423a-c9e1-2529fdd8650b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjCgh2QRW5Iu"
   },
   "outputs": [],
   "source": [
    "tokenizer = spm.SentencePieceProcessor(model_file='./spm.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBizS78pV5yQ"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VisionLanguageModel:\n\tsize mismatch for vision_encoder.pos_embedding: copying a param with shape torch.Size([1, 37, 512]) from checkpoint, the shape in current model is torch.Size([1, 197, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     10\u001b[39m model = VisionLanguageModel(\n\u001b[32m     11\u001b[39m     n_embed=n_embed,\n\u001b[32m     12\u001b[39m     img_embed_dim=image_embed_dim,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     num_labels = \u001b[32m4\u001b[39m\n\u001b[32m     22\u001b[39m )\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# model.to(device)\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# checkpoint = torch.load(best_ckpt_path)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./model_encoder.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m model.eval()  \u001b[38;5;66;03m# set to eval mode if you're going to do inference\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Load image\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for VisionLanguageModel:\n\tsize mismatch for vision_encoder.pos_embedding: copying a param with shape torch.Size([1, 37, 512]) from checkpoint, the shape in current model is torch.Size([1, 197, 512])."
     ]
    }
   ],
   "source": [
    "n_embed, num_hiddens, num_heads, n_layer = 128, 512, 8, 8\n",
    "image_embed_dim = num_hiddens\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "num_blocks = 2\n",
    "\n",
    "n_layer, block_size, num_hiddens = 8, 32, 512\n",
    "\n",
    "# Initialize the model\n",
    "model = VisionLanguageModel(\n",
    "    n_embed=n_embed,\n",
    "    img_embed_dim=image_embed_dim,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_layer=n_layer,\n",
    "    img_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    num_heads=num_heads,\n",
    "    num_blocks=num_blocks,\n",
    "    emb_dropout=0.1,\n",
    "    block_dropout=0.1,\n",
    "    num_labels = 4\n",
    ")\n",
    "# model.to(device)\n",
    "# checkpoint = torch.load(best_ckpt_path)\n",
    "model.load_state_dict(torch.load(\"./model_encoder.pth\"))\n",
    "model.eval()  # set to eval mode if you're going to do inference\n",
    "\n",
    "# Load image\n",
    "img_path = './image-1d100e9.jpg'  # 🔁 Replace with your actual image path\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# Preprocessing image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # make sure this matches ViT input\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "img_tensor = transform(image).unsqueeze(0)  # shape: [1, 3, 224, 224]\n",
    "\n",
    "# Move img_tensor to the same device as the model\n",
    "# img_tensor = img_tensor.to(device)\n",
    "\n",
    "# Prepare prompt and tokenize it\n",
    "prompt = \"Question: What do respiration and combustion give out\\nChoices:\\n0. Oxygen\\n1. Carbon dioxide\\n2. Nitrogen\\n3. Heat\\nAnswer with the number.\"\n",
    "tokens = tokenizer.encode(prompt)\n",
    "\n",
    "# Convert tokens list to a PyTorch tensor\n",
    "tokens_tensor = torch.tensor(tokens).unsqueeze(0) # Add batch dimension and move to device\n",
    "\n",
    "# --- 4. Run inference ---\n",
    "with torch.no_grad():\n",
    "    output_tokens = model(\n",
    "        img_array=img_tensor,\n",
    "        idx=tokens_tensor,  # Pass tensor instead of list\n",
    "        # max_new_tokens=50\n",
    "    )\n",
    "    predictions = torch.argmax(output_tokens, dim=-1)  # shape: (batch_size, seq_len)   '\n",
    "    # print(predictions.shape)\n",
    "    # print(tokens_tensor.shape)\n",
    "    print(predictions)\n",
    "    # out = torch.cat([tokens_tensor, predictions], dim=1)\n",
    "    # print(out.view(-1).shape)\n",
    "    # print(out.view(-1))\n",
    "    # print(predictions)\n",
    "    # predictions = output_tokens\n",
    "\n",
    "    # Convert the tensor to a list and filter out pad_id\n",
    "    # output_tokens_list = [token.item() for token in predictions.flatten() if token.item() != tokenizer.pad_id()]\n",
    "    # print(tokenizer.decode(output_tokens_list))\n",
    "    # # print(output_tokens.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1744965288569,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "h9QzGJLfBnSg",
    "outputId": "91da19cb-8e3b-4f71-f934-9e701720edb5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Decode and handle special tokens ---\n",
    "# Convert tensor to list and decode\n",
    "output_tokens_list = output_tokens[0].cpu().numpy().tolist()\n",
    "\n",
    "print(output_tokens_list)\n",
    "\n",
    "# # Remove special tokens manually (if needed)\n",
    "# # For example, let's assume that 0 is the token for padding (common in many models)\n",
    "# # Modify the list to remove any special tokens, if necessary\n",
    "# output_tokens_list = [token for token in output_tokens_list if token != tokenizer.pad_id()]\n",
    "\n",
    "# # Now, decode the remaining tokens\n",
    "# output_text = tokenizer.decode(output_tokens_list)\n",
    "# print(\"Answer:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JA3bwtEDspaj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00dbc84d65d14669af49353af2fe81c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "017b1fd478bc493c9eae5aafc36ca0ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e19b6faf003d4d458b22ae755644dd4e",
       "IPY_MODEL_cc0b64d960f34603b9a191b1e55eddef",
       "IPY_MODEL_50b7ca0016e74b9b86b406a58f0eb37a"
      ],
      "layout": "IPY_MODEL_42a90f9929544cbb9b3540878f1ab9ff"
     }
    },
    "0f46dd8c14094ae19910136bb2ec19a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_202fee786bf74b198c48307ede3c55c6",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2a00aa99569f4bff8d506a6a282624f1",
      "value": 100
     }
    },
    "202fee786bf74b198c48307ede3c55c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a00aa99569f4bff8d506a6a282624f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3d8c3f01e1034f129e1e06e17bad751f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3df75f9d77464f19b10483b1b04468f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f2e003ba2e445658cf925566c53afe8",
      "placeholder": "​",
      "style": "IPY_MODEL_cdc95decd0694b5396f08e64aed715ae",
      "value": " 2434/2434 [00:35&lt;00:00, 72.35 examples/s]"
     }
    },
    "42a90f9929544cbb9b3540878f1ab9ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43ae542de31241efbf99b13b7b043359": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45d13ed6fef54059ae2c0c62f2e3481d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a85684c8ea1c41288d8e4a4d4b5c6b1f",
       "IPY_MODEL_0f46dd8c14094ae19910136bb2ec19a6",
       "IPY_MODEL_aacec83f466a43779ecc05ba9fada696"
      ],
      "layout": "IPY_MODEL_e60330e7e3704f119f666acb11fb93be"
     }
    },
    "45d1881f95724550bf82bc6bc85d19fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b38adec2ad1413c8b8dabcd41a3d9ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50b7ca0016e74b9b86b406a58f0eb37a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_704f403601be451db32201221aba1b78",
      "placeholder": "​",
      "style": "IPY_MODEL_fa7f6a094ac349f2b39e59675f1d6539",
      "value": " 2434/2434 [00:34&lt;00:00, 71.16 examples/s]"
     }
    },
    "5c46284f51b7416dacae0d9a67ee986c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_637ef79ba39b4eb18061ca44840569e4",
      "max": 2434,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b31bca7a1ee94fb780cf90a8ab4bfb52",
      "value": 2434
     }
    },
    "637ef79ba39b4eb18061ca44840569e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "704f403601be451db32201221aba1b78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7aab307a4aa44c1a9f439089284a2a30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f2e003ba2e445658cf925566c53afe8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97570c5dc77d4ebf92007397ddf01d64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6ced33ea5a04e5aae45deb1db6939df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a85684c8ea1c41288d8e4a4d4b5c6b1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee9e868961f64ab5843beb0aa6841216",
      "placeholder": "​",
      "style": "IPY_MODEL_3d8c3f01e1034f129e1e06e17bad751f",
      "value": "Map: 100%"
     }
    },
    "aacec83f466a43779ecc05ba9fada696": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbac3bf0c58a417996f62f2f2dd58c4b",
      "placeholder": "​",
      "style": "IPY_MODEL_7aab307a4aa44c1a9f439089284a2a30",
      "value": " 100/100 [00:20&lt;00:00,  1.61 examples/s]"
     }
    },
    "add9d266ded843afa4819be93e994c03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b31bca7a1ee94fb780cf90a8ab4bfb52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c49155ad5e0744d3b840c444f87fe4a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45d1881f95724550bf82bc6bc85d19fd",
      "placeholder": "​",
      "style": "IPY_MODEL_97570c5dc77d4ebf92007397ddf01d64",
      "value": "Filter: 100%"
     }
    },
    "cbac3bf0c58a417996f62f2f2dd58c4b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc0b64d960f34603b9a191b1e55eddef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b38adec2ad1413c8b8dabcd41a3d9ad",
      "max": 2434,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_add9d266ded843afa4819be93e994c03",
      "value": 2434
     }
    },
    "cdc95decd0694b5396f08e64aed715ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e19b6faf003d4d458b22ae755644dd4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43ae542de31241efbf99b13b7b043359",
      "placeholder": "​",
      "style": "IPY_MODEL_a6ced33ea5a04e5aae45deb1db6939df",
      "value": "Filter: 100%"
     }
    },
    "e60330e7e3704f119f666acb11fb93be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee9e868961f64ab5843beb0aa6841216": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f03af258ff3f44b198a34a4dfffb99ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c49155ad5e0744d3b840c444f87fe4a3",
       "IPY_MODEL_5c46284f51b7416dacae0d9a67ee986c",
       "IPY_MODEL_3df75f9d77464f19b10483b1b04468f0"
      ],
      "layout": "IPY_MODEL_00dbc84d65d14669af49353af2fe81c0"
     }
    },
    "fa7f6a094ac349f2b39e59675f1d6539": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
