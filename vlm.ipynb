{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109969,
     "status": "ok",
     "timestamp": 1745302976473,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "2m_Ec3B3SZVV",
    "outputId": "38b808ae-be9c-47c3-8762-4db3453b38c0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.env/lib/python3.12/site-packages (3.5.0)\n",
      "Requirement already satisfied: torchvision in ./.env/lib/python3.12/site-packages (0.22.0)\n",
      "Requirement already satisfied: sentencepiece in ./.env/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in ./.env/lib/python3.12/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.env/lib/python3.12/site-packages (from datasets) (2.2.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.env/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.env/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.env/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.env/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.env/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.env/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.env/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in ./.env/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in ./.env/lib/python3.12/site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.env/lib/python3.12/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in ./.env/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: torch==2.7.0 in ./.env/lib/python3.12/site-packages (from torchvision) (2.7.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.env/lib/python3.12/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.env/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (4.13.2)\n",
      "Requirement already satisfied: setuptools in ./.env/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (79.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.env/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./.env/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.env/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.env/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.env/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.env/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.env/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.env/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.env/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.env/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.env/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.env/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.env/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.env/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.env/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.env/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.env/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in ./.env/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.env/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.env/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.env/lib/python3.12/site-packages (from jinja2->torch==2.7.0->torchvision) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets torchvision sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 14132,
     "status": "ok",
     "timestamp": 1745302995197,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "7veYguFjspaV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import Tuple\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7J9Pd71spaW"
   },
   "source": [
    "## Converting Image to a Sequence of Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1745303000415,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "1-lrtKNFspaX"
   },
   "outputs": [],
   "source": [
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self, img_size: int = 96, patch_size: int = 16, hidden_dim: int = 512\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # Store the input image size, the patch size and hidden dimension\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Calculate the total number of patches\n",
    "        self.num_patches = (self.img_size // self.patch_size) ** 2\n",
    "\n",
    "        # Create a convolution to extract patch embeddings\n",
    "        # in_channels=3 asummes a 3-channel image (RGB)\n",
    "        # outp_channels=hidden_dim sets the number of output channels to match the hidden dimension\n",
    "        # kernel_size=patch_size and stride=patch_size ensuring each patch is embedded separately\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=self.hidden_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        X = self.conv(X)\n",
    "\n",
    "        # Flatten the spatial dimensions (height and width) of the patch embeddings\n",
    "        # This step flattens the patch dimensions to a single dimension\n",
    "        # Output shape: (batch_size, hidden_dim, self.num_patches)\n",
    "        X = X.flatten(2)\n",
    "\n",
    "        # Transpose the dimensions to obtain the shape (batch_size, num_patches, hidden_dim)\n",
    "        # This step brings the num_patches dimension to the second position\n",
    "        # Output shape: (batch_size, self.num_patches, hidden_dim)\n",
    "        X = X.transpose(1, 2)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1745303004155,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "79pfUClUspaY",
    "outputId": "7fc1cabf-ad31-4f63-c65d-871940a9f024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image patches: torch.Size([128, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "B, C, H, W = 128, 3, 96, 96  # Batch size, Channels, Height, Width\n",
    "X = torch.randn(B, C, H, W)\n",
    "\n",
    "patch_size = 16\n",
    "hidden_dim = 512\n",
    "\n",
    "patch_embeddings = PatchEmbeddings(\n",
    "    img_size=H, patch_size=patch_size, hidden_dim=hidden_dim\n",
    ")\n",
    "patches = patch_embeddings(X)\n",
    "print(f\"Shape of image patches: {patches.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1745303005332,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "NTHJZgh0spaY",
    "outputId": "3facf4a2-5bd9-4d80-e6a2-bc33eb1645b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "num_patches = (H // patch_size) ** 2\n",
    "assert patches.shape == (B, num_patches, hidden_dim), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgQvJSuFspaZ"
   },
   "source": [
    "## Attention Mechanism\n",
    "Attention Mechanism across both the vision encoder and language decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xrdt5oNPspaZ"
   },
   "source": [
    "### The implementation of the Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1745303007120,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "_Zuq1aL5spaZ"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        head_size: int,\n",
    "        dropout: float = 0.1,\n",
    "        is_decoder: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear layer for Key projection\n",
    "        self.key = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "\n",
    "        # Linear layer for Query projection\n",
    "        self.query = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "\n",
    "        # Linear layer for Value projection\n",
    "        self.value = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "\n",
    "        # Dropout layer for regularization to prevent overfitting\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Flag indicating wheter the head is used as a decoder\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Get batch size (B), sequence length (T), and embedding dimension (C) from the input tensor\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Compute Key, Query, and Value projections\n",
    "        k = self.key(x)  # Shape: (B, T, head_size)\n",
    "        q = self.query(x)  # Shape: (B, T, head_size)\n",
    "        v = self.value(x)  # SHape: (B, T, head_size)\n",
    "        wei = q @ k.transpose(-2, -1) * (C**-0.5)  # Shape: (B, T, T)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            tril = torch.tril(torch.ones(T, T, dtype=torch.bool, device=x.device))\n",
    "            wei = wei.masked_fill(mask=tril == 0, value=float(\"-inf\"))\n",
    "\n",
    "        wei = F.softmax(input=wei, dim=-1)  # Shape: (B, T, T)\n",
    "\n",
    "        # Apply Dropout to the attention probabilities for regularization\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Perform a weighted aggregation of values using the attention probabilities\n",
    "        out = wei @ v  # Shape: (B, T, head_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1745303013537,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "JzOfIfuwspaa",
    "outputId": "3a0facc5-d1a5-4656-ccd4-aa1cc436b3c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([128, 36, 16])\n"
     ]
    }
   ],
   "source": [
    "B, T, C = patches.shape  # Batch size, Sequence length, Embedding dimension\n",
    "head_size = 16  # Size of the attention head\n",
    "\n",
    "head = Head(n_embed=C, head_size=head_size)\n",
    "output = head(patches)\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1745303015496,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "eVTRkWHqspaa",
    "outputId": "3dbf5216-edc7-43d9-e45a-b490b21b0192"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, head_size), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmXF3DIhspaa"
   },
   "source": [
    "### The implementation of Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1745303017117,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "inh7qmxnspab"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.1,\n",
    "        is_decoder: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensure that the embedding dimension is divisible by the number of heads\n",
    "        assert n_embed % num_heads == 0, \"n_embed must be divisible by num_heads!\"\n",
    "\n",
    "        # Create a ModuleList of attention heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            modules=[\n",
    "                Head(\n",
    "                    n_embed=n_embed,\n",
    "                    head_size=n_embed // num_heads,\n",
    "                    dropout=dropout,\n",
    "                    is_decoder=is_decoder,\n",
    "                )\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Linear layer for projecting the concatenated head outputs\n",
    "        self.proj = nn.Linear(in_features=n_embed, out_features=n_embed)\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply each attention head to the input tensor\n",
    "        head_outputs = [\n",
    "            h(x) for h in self.heads\n",
    "        ]  # Shape: num_heads * (B, T, head_size)\n",
    "\n",
    "        # Concatenate the outputs from all heads along the last dimension\n",
    "        out = torch.cat(tensors=head_outputs, dim=-1)  # Shape: (B, T, m_embed)\n",
    "\n",
    "        # Apply the projection layer to the concatenated outputs\n",
    "        out = self.proj(out)  # Shape: (B, T, m_embed)\n",
    "\n",
    "        # Apply Dropout to the projected outputs for regularization\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1745303018449,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "4kRG1mj7spab"
   },
   "outputs": [],
   "source": [
    "num_heads = 2\n",
    "dropout = 0.1\n",
    "mha = MultiHeadAttention(n_embed=C, num_heads=num_heads, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1745303019397,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "7oaAkaZ7spab",
    "outputId": "edcb66de-0bac-48a3-a5dd-7fa3f498577d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([128, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "output = mha(patches)\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1745303020699,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "0I5qysIxspab",
    "outputId": "ae2793fc-fdca-424d-ae91-5d23e4855d6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, C), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ona5tht3spac"
   },
   "source": [
    "### The Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745303022869,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "1QPxO9cWspac"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_embed: int, dropout: float = 0.1, is_decoder: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the layers of the MLP\n",
    "        layers = [\n",
    "            # First linear layer that expands the input dimension from n_embed to 4 * n_embed\n",
    "            nn.Linear(in_features=n_embed, out_features=4 * n_embed),\n",
    "            # Activation function: ReLU if is_decoder is True, else GELU\n",
    "            nn.ReLU() if is_decoder else nn.GELU(),\n",
    "            # Second linear layer that projects the intermediate dimension back to n_embed\n",
    "            nn.Linear(in_features=4 * n_embed, out_features=n_embed),\n",
    "            # Dropout layer for regularization\n",
    "            nn.Dropout(p=dropout),\n",
    "        ]\n",
    "\n",
    "        # Create the MLP as a sequence of layers\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Pass the input through the MLP layers\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1745303024553,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "BK7FPIPyspac"
   },
   "outputs": [],
   "source": [
    "dropout = 0.1\n",
    "mlp = MLP(n_embed=C, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1745303025568,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "AF3WnIsospac",
    "outputId": "312131d6-dd21-4e99-8d6f-2562502a136a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([1, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "output = mlp(output)  # Previous output of the Multihead Attention\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1745303026688,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "0Gj0NrF8spad",
    "outputId": "31f9ad8f-1f94-4d50-aed2-8cce238d2d72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, C), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6888PMDpspad"
   },
   "source": [
    "### Transformer Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1745303028511,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "jLy1uQa2spad"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.1,\n",
    "        is_decoder: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Layer normalization for the input to the attention layer\n",
    "        self.ln1 = nn.LayerNorm(normalized_shape=n_embed)\n",
    "\n",
    "        # Multi-head attention module\n",
    "        self.mhattn = MultiHeadAttention(\n",
    "            n_embed=n_embed, num_heads=num_heads, dropout=dropout, is_decoder=is_decoder\n",
    "        )\n",
    "\n",
    "        # Layer normalization for the input to the FFN\n",
    "        self.ln2 = nn.LayerNorm(normalized_shape=n_embed)\n",
    "\n",
    "        # Feed-forward neural network (FFN)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_features=n_embed, out_features=4 * n_embed),\n",
    "            nn.GELU(),  # Activation function\n",
    "            nn.Linear(\n",
    "                in_features=4 * n_embed, out_features=n_embed\n",
    "            ),  # Projection back to the original dimension\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Saving the input for residual connection\n",
    "        original_x = x\n",
    "\n",
    "        # Apply layer normalization to the input\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        # Apply multi-head attention\n",
    "        mhattn_output = self.mhattn(x)\n",
    "\n",
    "        # Add the residual connection (original input) to the attention output\n",
    "        x = original_x + mhattn_output\n",
    "\n",
    "        # Apply later normalization to the input to the FFN\n",
    "        x = self.ln2(x)\n",
    "\n",
    "        # Apply the FFN\n",
    "        ffn_output = self.ffn(x)\n",
    "\n",
    "        # Apply the residual connection (input to the FFN) to the FFN output\n",
    "        x = x + ffn_output\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1745303029987,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "trl5mtK5spad"
   },
   "outputs": [],
   "source": [
    "num_heads = 2\n",
    "dropout = 0.1\n",
    "block = Block(n_embed=C, num_heads=num_heads, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745303031188,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "xZMaSLSCspad",
    "outputId": "072fba58-82ed-4adb-dc7d-0eea71693846"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([1, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "output = block(patches)\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1745303032249,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "POugdyswspad",
    "outputId": "c467b86a-b268-4fdb-b1ac-08cd975e6327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, C), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h31vzsbQspae"
   },
   "source": [
    "## Vision Encoder - Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DNEtMj-spae"
   },
   "source": [
    "Combining patchification logic and attention block in to ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745303034064,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "YGX1s-qCspae"
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int,\n",
    "        patch_size: int,\n",
    "        num_hiddens: int,\n",
    "        num_heads: int,\n",
    "        num_blocks: int,\n",
    "        emb_dropout: float,\n",
    "        block_dropout: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding layer to convert the input image into patches\n",
    "        self.patch_embedding = PatchEmbeddings(\n",
    "            img_size=img_size, patch_size=patch_size, hidden_dim=num_hiddens\n",
    "        )\n",
    "\n",
    "        # Learnable classification token\n",
    "        self.cls_token = nn.Parameter(data=torch.zeros(size=(1, 1, num_hiddens)))\n",
    "\n",
    "        # Calculate the number of patches\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Learnable position embedding\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            data=torch.randn(size=(1, num_patches + 1, num_hiddens))\n",
    "        )\n",
    "\n",
    "        # Dropout layer for the embeddings\n",
    "        self.dropout = nn.Dropout(p=emb_dropout)\n",
    "\n",
    "        # Stack of transformer blocks\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    n_embed=num_hiddens,\n",
    "                    num_heads=num_heads,\n",
    "                    dropout=block_dropout,\n",
    "                    is_decoder=False,\n",
    "                )\n",
    "                for _ in range(num_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Layer normalization for the final representation\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=num_hiddens)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        # Convert the input image into patch embeddings\n",
    "        x = self.patch_embedding(X)  # Shape: (B, num_patches, num_hiddens)\n",
    "\n",
    "        # Expand the classification token to match the batch size\n",
    "        cls_tokens = self.cls_token.expand(\n",
    "            x.shape[0], -1, -1\n",
    "        )  # Shape: (B, 1, num_hiddens)\n",
    "\n",
    "        # Concatenate the classification token with the patch embeddings\n",
    "        x = torch.cat(\n",
    "            tensors=(cls_tokens, x), dim=1\n",
    "        )  # Shape: (B, num_patches + 1, num_hiddens)\n",
    "\n",
    "        # Add the position embedding to the patch embeddings\n",
    "        x += self.pos_embedding  # Shape: (B, num_patches + 1, num_hiddens)\n",
    "\n",
    "        # Apply dropout to the embeddings\n",
    "        x = self.dropout(x)  # Shape: (B, num_patches + 1, num_hiddens)\n",
    "\n",
    "        # Pass the embeddings through the transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)  # Shape: (B, num_patches + 1, num_hiddens)\n",
    "\n",
    "        # Apply layer normalization to the `[CLS]` token's final representation\n",
    "        x = self.layer_norm(x[:, 0])  # Shape: (B, num_hiddens)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745303036549,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "cYwJwvrjspae"
   },
   "outputs": [],
   "source": [
    "B, C, H, W = 2, 3, 96, 96  # Batch size, Channels, Height, Width\n",
    "X = torch.randn(B, C, H, W)\n",
    "vit = ViT(\n",
    "    img_size=H,\n",
    "    patch_size=16,\n",
    "    num_hiddens=64,\n",
    "    num_heads=2,\n",
    "    num_blocks=2,\n",
    "    emb_dropout=0.1,\n",
    "    block_dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1745303037969,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "GVwEBk8Sspae",
    "outputId": "0288437a-38f5-461c-df7d-d82273687366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "output = vit(X)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1745303039213,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "OCrJkhG2spaf",
    "outputId": "b48d4ff4-e87d-4368-d86b-ccdd8ab5e2f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, 64), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1teH1MPspaf"
   },
   "source": [
    "## Vision-Language Projection Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EuAJV2rspaf"
   },
   "source": [
    "Unfortunatelly, we can't directly concatenate ViT output to the text embeddings. <br>\n",
    "We need to project this from dimensionality of image embeddings from the vision transformer to the dimensionality of text embeddings.\n",
    "\n",
    "Why MLP for this part? If you want to train VLM with low resources you can do so by keeping both the pretrained vision encoder and language decoder frozen during the VLM training. Therefore, allocating more parameters to the connection module could enhance the overall VLM's ability to generalize and help in the downstream instruction-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1745303043943,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "fsAv9P6aspaf"
   },
   "outputs": [],
   "source": [
    "class MultiModalProjector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        img_embed_dim: int,\n",
    "        dropout: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the projection network\n",
    "        self.net = nn.Sequential(\n",
    "            # Linear layer to expand the image embedding dimension\n",
    "            nn.Linear(in_features=img_embed_dim, out_features=4 * img_embed_dim),\n",
    "            # GELU activation function\n",
    "            nn.GELU(),\n",
    "            # Linear layer to project the expanded image embeddings to the text embedding dimension\n",
    "            nn.Linear(in_features=4 * img_embed_dim, out_features=n_embed),\n",
    "            # Dropout layer for regularization\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Pass the input through the projection network\n",
    "        x = self.net(x)  # Shape: (B, img_embed_dim) --> (B, n_embed)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1745303045845,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "l7ZYNX_sspaf"
   },
   "outputs": [],
   "source": [
    "B, n_embed, img_embed_dim = 2, 64, 128\n",
    "X = torch.randn(size=(B, img_embed_dim))\n",
    "\n",
    "projector = MultiModalProjector(\n",
    "    n_embed=n_embed, img_embed_dim=img_embed_dim, dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745303046992,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "TR2UZ-Kgspaf",
    "outputId": "387d1e35-1629-4f3c-91ab-ed7485ea339c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "output = projector(X)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1745303048116,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "TYJa4bTqspaf",
    "outputId": "772a624c-8048-4b2a-d870-13f27f2ef4a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, n_embed), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYlkygxcspag"
   },
   "source": [
    "## Building the Decoder Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unGnK01Yspag"
   },
   "source": [
    "Only thing that deviates from origianl implementation is that here projection module is integrated into decoder model class. <br>\n",
    "In contrary, when using pretrained models with HuggingFace (or any other library), you can directly feed embeddings as input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745303049998,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "UMcEL2qvspag"
   },
   "outputs": [],
   "source": [
    "class DecoderLanguageModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        img_embed_dim: int,\n",
    "        vocab_size: int,\n",
    "        num_heads: int,\n",
    "        n_layer: int,\n",
    "        num_labels: int,\n",
    "        use_images: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_images = use_images\n",
    "\n",
    "        # Token embedding table\n",
    "        self.token_embedding_table = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=n_embed\n",
    "        )\n",
    "\n",
    "        # Position embedding table\n",
    "        self.position_embedding_table = nn.Embedding(\n",
    "            num_embeddings=1000, embedding_dim=n_embed\n",
    "        )\n",
    "\n",
    "        if use_images:\n",
    "            # Image projection layer to align image embeddings with text embeddings\n",
    "            self.image_projection = MultiModalProjector(\n",
    "                n_embed=n_embed, img_embed_dim=img_embed_dim\n",
    "            )\n",
    "\n",
    "        # Stack of transformer decoder blocks\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                Block(n_embed=n_embed, num_heads=num_heads, is_decoder=True)\n",
    "                for _ in range(n_layer)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(normalized_shape=n_embed)\n",
    "\n",
    "        # Language modeling head\n",
    "        self.lm_head = nn.Linear(in_features=n_embed, out_features=vocab_size)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            idx: torch.Tensor,\n",
    "            img_embeds: torch.Tensor = None,\n",
    "            targets: torch.Tensor = None,\n",
    "        ) -> torch.Tensor:\n",
    "            # Get token embeddings from the input indices\n",
    "            tok_emb = self.token_embedding_table(idx)\n",
    "\n",
    "            if self.use_images:\n",
    "                # Project and concatenate image embeddings with token embeddings\n",
    "                img_emb = self.image_projection(img_embeds).unsqueeze(1)\n",
    "                tok_emb = torch.cat([img_emb, tok_emb], dim=1)\n",
    "\n",
    "            # Get position embeddings\n",
    "            pos_emb = self.position_embedding_table(\n",
    "                torch.arange(tok_emb.size(1), device=idx.device)\n",
    "            )\n",
    "\n",
    "            # Add position embeddings to token embeddings\n",
    "            x = tok_emb + pos_emb\n",
    "\n",
    "            # Pass through the transformer decoder blocks\n",
    "            x = self.blocks(x)\n",
    "\n",
    "            # Apply final layer normalization\n",
    "            x = self.ln_f(x)\n",
    "\n",
    "            # Get the logits from the language modeling head\n",
    "            logits = self.lm_head(x)\n",
    "\n",
    "\n",
    "\n",
    "            return logits\n",
    "\n",
    "    def generate(\n",
    "        self, idx: torch.Tensor, img_embeds: torch.Tensor, max_new_tokens: int\n",
    "    ) -> torch.Tensor:\n",
    "        B, T = idx.shape\n",
    "\n",
    "        generated = idx\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "\n",
    "        if self.use_images and img_embeds is not None:\n",
    "            img_emb = self.image_projection(img_embeds).unsqueeze(1)\n",
    "            current_output = torch.cat([img_emb, tok_emb], dim=1)\n",
    "        else:\n",
    "            current_output = tok_emb\n",
    "        for i in range(max_new_tokens):\n",
    "            T_current = current_output.shape[1]\n",
    "\n",
    "            current_pos_emb = self.position_embedding_table(\n",
    "                torch.arange(T_current, device=idx.device)\n",
    "            ).unsqueeze(0)\n",
    "\n",
    "            current_output += current_pos_emb\n",
    "\n",
    "            for block in self.blocks:\n",
    "                current_output = block(current_output)\n",
    "\n",
    "            logits = self.lm_head(current_output[:, -1, :])\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            idx_next = torch.multinomial(input=probs, num_samples=1)\n",
    "\n",
    "            generated = torch.cat([generated, idx_next], dim=1)\n",
    "\n",
    "            idx_next_emb = self.token_embedding_table(idx_next)\n",
    "\n",
    "            current_output = torch.cat([current_output, idx_next_emb], dim=1)\n",
    "\n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1745303051615,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "DziYVmmNxU1b",
    "outputId": "945ac5bc-e779-47aa-d68e-177e4e157b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.9884,  0.4888, -0.6352, -0.8872, -0.8286,  0.9817, -0.9919,\n",
      "           2.2992,  0.4834, -0.2820],\n",
      "         [-1.2395,  0.4930, -0.4717,  0.0180, -0.0817, -0.7556,  1.3953,\n",
      "           1.5507, -1.0333,  0.7443],\n",
      "         [-1.8063,  0.8628,  0.8860,  1.2261, -1.4704, -1.0956,  0.8466,\n",
      "           0.4960, -0.5536, -0.5312],\n",
      "         [-2.0383,  2.1994,  1.4705,  0.3720,  1.4597, -0.2057,  1.1792,\n",
      "          -0.1191,  0.1260, -0.4278],\n",
      "         [ 0.4250,  0.3479,  1.6227,  0.5459, -1.2226, -0.8604,  1.0797,\n",
      "          -0.0270, -1.0866,  0.5499]],\n",
      "\n",
      "        [[ 1.0782,  1.4777, -1.8177, -1.0967,  0.9454, -0.5596, -1.0864,\n",
      "           0.4142, -0.8863,  0.5891],\n",
      "         [ 0.6128, -0.0598,  0.2060,  2.5046, -1.1826, -0.2640,  0.5922,\n",
      "          -1.7396, -0.0711, -0.4729],\n",
      "         [-1.6782, -0.2271, -0.6554,  0.2233,  0.7869,  1.0556, -0.9542,\n",
      "           0.8588,  0.3978,  0.0564],\n",
      "         [ 0.0280,  0.6646,  0.8989,  0.7665, -0.2827, -0.9264, -0.6597,\n",
      "           0.6160,  0.2149, -0.1791],\n",
      "         [-1.1882,  0.3055, -0.7482,  0.0933,  0.0244, -0.0126, -0.3636,\n",
      "          -0.1404, -0.7148,  1.1211]]])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10])\n",
      "Loss: 2.668959379196167\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simulated inputs\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "vocab_size = 10\n",
    "\n",
    "# Fake target labels (batch_size x seq_len)\n",
    "targets = torch.tensor([\n",
    "    [1, 2, 3, 4],\n",
    "    [2, 3, 4, 5]\n",
    "], dtype=torch.long)  # shape: [2, 4]\n",
    "\n",
    "# Dummy index tensor for alignment (same batch_size)\n",
    "idx = torch.arange(batch_size)\n",
    "\n",
    "# Simulated logits output from a model (batch_size x seq_len+1 x vocab_size)\n",
    "# We assume image embeddings are used, so seq_len + 1\n",
    "logits = torch.randn(batch_size, seq_len + 1, vocab_size)  # shape: [2, 5, 10]\n",
    "print(logits)\n",
    "\n",
    "# Add dummy target token for image embedding (-100 will be ignored in loss)\n",
    "targets = torch.cat(\n",
    "    [\n",
    "        torch.full(\n",
    "            (batch_size, 1), -100, dtype=torch.long, device=targets.device\n",
    "        ),\n",
    "        targets\n",
    "    ],\n",
    "    dim=1\n",
    ")  # shape: [2, 5]\n",
    "\n",
    "print(logits.view(-1, vocab_size).shape)\n",
    "print(targets.view(-1).shape)\n",
    "\n",
    "# Confirm shapes match\n",
    "assert logits.shape[:2] == targets.shape, f\"Shape mismatch: {logits.shape[:2]} vs {targets.shape}\"\n",
    "\n",
    "# Compute cross-entropy loss\n",
    "loss = F.cross_entropy(\n",
    "    input=logits.view(-1, vocab_size),  # shape: [2*5, 10]\n",
    "    target=targets.view(-1),            # shape: [2*5]\n",
    "    ignore_index=-100\n",
    ")\n",
    "\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1745303053121,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "eMfZ5bfSspag",
    "outputId": "85228429-d569-483d-c5f9-2ab36d6e151c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB_uIrRsspag"
   },
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1786,
     "status": "ok",
     "timestamp": 1745303059188,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "dOeSBdLCspag",
    "outputId": "438d6160-7564-4c88-f3dc-b2a576240036"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Test forward pass\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Check if you need to calculate loss by providing targets\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     logits, loss = model(idx, image_embeds, targets)\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLogits shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "n_embed, img_embed_dim, vocab_size, num_heads, n_layer = 128, 256, 1000, 8, 6\n",
    "# `n_layer` is used to represent number of decoder transformer blocks and num_blocks for the vision encoder to avoid confusion\n",
    "model = DecoderLanguageModel(\n",
    "    n_embed=n_embed,\n",
    "    img_embed_dim=img_embed_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    num_heads=num_heads,\n",
    "    n_layer=n_layer,\n",
    "    use_images=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Dummy input\n",
    "B, T = 10, 50\n",
    "idx = torch.randint(low=0, high=vocab_size, size=(B, T)).to(device)\n",
    "image_embeds = torch.randn(B, 256).to(device)  # Assume img_embed_dim is 256\n",
    "\n",
    "targets = torch.randint(0, vocab_size, (B, T)).to(\n",
    "    device\n",
    ")  # Only if you want to compute loss\n",
    "\n",
    "# Test forward pass\n",
    "# Check if you need to calculate loss by providing targets\n",
    "if targets is not None:\n",
    "    logits, loss = model(idx, image_embeds, targets)\n",
    "    print(f\"Logits shape: {logits.shape}, Loss: {loss}\")\n",
    "else:\n",
    "    logits = model(idx, image_embeds)  # Call without targets\n",
    "    print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "# Test generation\n",
    "generated = model.generate(idx, image_embeds, max_new_tokens=20)\n",
    "print(f\"Generated sequence shape: {generated.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93X7Cpv3spah"
   },
   "source": [
    "## Putting everything together: Simple Vision Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1745303062504,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "629X82iGspah"
   },
   "outputs": [],
   "source": [
    "class VisionLanguageModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        img_embed_dim: int,\n",
    "        vocab_size: int,\n",
    "        n_layer: int,\n",
    "        img_size: int,\n",
    "        patch_size: int,\n",
    "        num_heads: int,\n",
    "        num_blocks: int,\n",
    "        emb_dropout: float,\n",
    "        block_dropout: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Set num_hiddens equal to img_embed_dim\n",
    "        num_hiddens = img_embed_dim\n",
    "\n",
    "        # Assert that num_hiddens is divisible by num_heads\n",
    "        assert num_hiddens % num_heads == 0, ValueError(\n",
    "            \"num_hiddens must be divisible by num_heads!\"\n",
    "        )\n",
    "\n",
    "        # Initialize the Vision Transformer (ViT) encoder\n",
    "        self.vision_encoder = ViT(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            num_hiddens=num_hiddens,\n",
    "            num_heads=num_heads,\n",
    "            num_blocks=num_blocks,\n",
    "            emb_dropout=emb_dropout,\n",
    "            block_dropout=block_dropout,\n",
    "        )\n",
    "\n",
    "        # Initialize the Language Model Decoder (DecoderLanguageModel)\n",
    "        self.decoder = DecoderLanguageModel(\n",
    "            n_embed=n_embed,\n",
    "            img_embed_dim=img_embed_dim,\n",
    "            vocab_size=vocab_size,\n",
    "            num_heads=num_heads,\n",
    "            n_layer=n_layer,\n",
    "            use_images=True,\n",
    "        )\n",
    "\n",
    "    def _check_image_embeddings(self, image_embeds: torch.Tensor) -> None:\n",
    "        \"\"\"Chek if image embeddings are valid.\"\"\"\n",
    "        if image_embeds.nelement() == 0 or image_embeds.shape[1] == 0:\n",
    "            raise ValueError(\n",
    "                \"Something is wrong with the ViT model. It's returning an empty tensor or the embedding dimension is empty.\"\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self, img_array: torch.Tensor, idx: torch.Tensor, targets: torch.Tensor = None\n",
    "    ) -> torch.Tensor | Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Get the image embeddings from the Vision Encoder\n",
    "        image_embeds = self.vision_encoder(img_array)\n",
    "\n",
    "        # Check if image embeddings are valid\n",
    "        self._check_image_embeddings(image_embeds)\n",
    "\n",
    "        if targets is not None:\n",
    "            # If targets are provided, compute the logits and loss\n",
    "            logits, loss = self.decoder(idx, image_embeds, targets)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            # If targets are not provided, compute only the logits\n",
    "            logits = self.decoder(idx, image_embeds)\n",
    "            return logits\n",
    "\n",
    "    def generate(\n",
    "        self, img_array: torch.Tensor, idx: torch.Tensor, max_new_tokens: int\n",
    "    ) -> torch.Tensor:\n",
    "        # Get the image embeddings from the Vision Encoder\n",
    "        image_embeds = self.vision_encoder(img_array)\n",
    "\n",
    "        # Check if image embeddings are valid\n",
    "        self._check_image_embeddings(image_embeds)\n",
    "\n",
    "        # Generate new tokens using the Language Model Decoder\n",
    "        generated_tokens = self.decoder.generate(\n",
    "            idx=idx, img_embeds=image_embeds, max_new_tokens=max_new_tokens\n",
    "        )\n",
    "        return generated_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oK3RP2Sspai"
   },
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 520,
     "status": "ok",
     "timestamp": 1745303065738,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "olxCWjIbspai",
    "outputId": "670ca330-1ee2-49be-89f3-5abe42e7516c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n\u001b[32m     10\u001b[39m model = VisionLanguageModel(\n\u001b[32m     11\u001b[39m     n_embed=n_embed,\n\u001b[32m     12\u001b[39m     img_embed_dim=image_embed_dim,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     block_dropout=\u001b[32m0.1\u001b[39m,\n\u001b[32m     21\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m model.to(\u001b[43mdevice\u001b[49m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Create dummy data with correct dimensions\u001b[39;00m\n\u001b[32m     25\u001b[39m dummy_img = torch.randn(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, img_size, img_size).to(\n\u001b[32m     26\u001b[39m     device\n\u001b[32m     27\u001b[39m )  \u001b[38;5;66;03m# Correct shape for image input\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "n_embed, num_hiddens, vocab_size, num_heads, n_layer = 128, 512, 1000, 8, 8\n",
    "image_embed_dim = num_hiddens\n",
    "img_size = 96\n",
    "patch_size = 16\n",
    "num_blocks = 2\n",
    "\n",
    "n_layer, block_size, num_hiddens = 8, 32, 512\n",
    "\n",
    "# Initialize the model\n",
    "model = VisionLanguageModel(\n",
    "    n_embed=n_embed,\n",
    "    img_embed_dim=image_embed_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    n_layer=n_layer,\n",
    "    img_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    num_heads=num_heads,\n",
    "    num_blocks=num_blocks,\n",
    "    emb_dropout=0.1,\n",
    "    block_dropout=0.1,\n",
    ")\n",
    "\n",
    "# Create dummy data with correct dimensions\n",
    "dummy_img = torch.randn(1, 3, img_size, img_size).to(\n",
    "    device\n",
    ")  # Correct shape for image input\n",
    "dummy_idx = torch.randint(0, vocab_size, (1, block_size)).to(\n",
    "    device\n",
    ")  # Correct shape for text input\n",
    "\n",
    "# Forward pass to initialize all parameters\n",
    "try:\n",
    "    output = model(dummy_img, dummy_idx)  # Output for debugging\n",
    "    print(\"Output from initialization forward pass:\", output)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Runtime Error during forward pass: {str(e)}\")\n",
    "    print(\"Check layer configurations and input shapes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745303068888,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "A5hDtsN9spai",
    "outputId": "fe07c6d1-2772-48d5-c7d2-bd880d4ecdbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 33, 1000])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ph6-Ca37z5ai"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1745303159482,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "gIY0SvARSxKh"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = spm.SentencePieceProcessor(model_file='spm.model')\n",
    "max_len = 256  # giới hạn token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1745303160893,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "YYllzhImS9BU"
   },
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "017b1fd478bc493c9eae5aafc36ca0ed",
      "e19b6faf003d4d458b22ae755644dd4e",
      "cc0b64d960f34603b9a191b1e55eddef",
      "50b7ca0016e74b9b86b406a58f0eb37a",
      "42a90f9929544cbb9b3540878f1ab9ff",
      "43ae542de31241efbf99b13b7b043359",
      "a6ced33ea5a04e5aae45deb1db6939df",
      "4b38adec2ad1413c8b8dabcd41a3d9ad",
      "add9d266ded843afa4819be93e994c03",
      "704f403601be451db32201221aba1b78",
      "fa7f6a094ac349f2b39e59675f1d6539"
     ]
    },
    "executionInfo": {
     "elapsed": 35730,
     "status": "ok",
     "timestamp": 1745303222813,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "-zyZX5q_S-DY",
    "outputId": "1623872e-3a22-4215-fa1b-b04312b5387e"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"HuggingFaceM4/the_cauldron\", \"ai2d\", split=\"train\")\n",
    "\n",
    "# Lọc những entry có ảnh và text hợp lệ\n",
    "dataset = dataset.filter(lambda x: x[\"images\"] and x[\"texts\"] and \"user\" in x[\"texts\"][0])\n",
    "\n",
    "# # Lấy 100 sample đầu tiên, lấy full dataset thì bỏ qua dòng này\n",
    "# dataset = dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "f03af258ff3f44b198a34a4dfffb99ea",
      "c49155ad5e0744d3b840c444f87fe4a3",
      "5c46284f51b7416dacae0d9a67ee986c",
      "3df75f9d77464f19b10483b1b04468f0",
      "00dbc84d65d14669af49353af2fe81c0",
      "45d1881f95724550bf82bc6bc85d19fd",
      "97570c5dc77d4ebf92007397ddf01d64",
      "637ef79ba39b4eb18061ca44840569e4",
      "b31bca7a1ee94fb780cf90a8ab4bfb52",
      "7f2e003ba2e445658cf925566c53afe8",
      "cdc95decd0694b5396f08e64aed715ae",
      "45d13ed6fef54059ae2c0c62f2e3481d",
      "a85684c8ea1c41288d8e4a4d4b5c6b1f",
      "0f46dd8c14094ae19910136bb2ec19a6",
      "aacec83f466a43779ecc05ba9fada696",
      "e60330e7e3704f119f666acb11fb93be",
      "ee9e868961f64ab5843beb0aa6841216",
      "3d8c3f01e1034f129e1e06e17bad751f",
      "202fee786bf74b198c48307ede3c55c6",
      "2a00aa99569f4bff8d506a6a282624f1",
      "cbac3bf0c58a417996f62f2f2dd58c4b",
      "7aab307a4aa44c1a9f439089284a2a30"
     ]
    },
    "executionInfo": {
     "elapsed": 57214,
     "status": "ok",
     "timestamp": 1744964343594,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "00mLCaeIuqXb",
    "outputId": "2812104e-868f-44f2-df87-d04acb19c1f0"
   },
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess(example):\n",
    "    img_data = example[\"images\"][0]\n",
    "\n",
    "    # Đảm bảo ảnh là PIL.Image\n",
    "    if isinstance(img_data, Image.Image):\n",
    "        img = img_data.convert(\"RGB\")\n",
    "    elif isinstance(img_data, np.ndarray):\n",
    "        img = Image.fromarray(img_data).convert(\"RGB\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported image format: {type(img_data)}\")\n",
    "\n",
    "    # Transform để ra Tensor\n",
    "    image = image_transform(img)  # Tensor (3, 224, 224)\n",
    "\n",
    "    # Tokenize prompt và target\n",
    "    prompt = example[\"texts\"][0][\"user\"]\n",
    "    target = example[\"texts\"][0].get(\"assistant\", \"\")\n",
    "\n",
    "    # full_input = prompt + \"\\n\" + target if target else prompt\n",
    "\n",
    "    pad_id = tokenizer.pad_id() if tokenizer.pad_id() >= 0 else 0\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    tokens = tokens[:max_len]\n",
    "    tokens += [pad_id] * (max_len - len(tokens))\n",
    "    input_ids = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "    # Tokenize target\n",
    "    if target:\n",
    "        target_tokens = tokenizer.encode(target)\n",
    "        target_tokens = target_tokens[:max_len]\n",
    "        target_tokens += [pad_id] * (max_len - len(target_tokens))\n",
    "        target_ids = torch.tensor(target_tokens, dtype=torch.long)\n",
    "    else:\n",
    "        target_ids = torch.full_like(input_ids, fill_value=pad_id)\n",
    "\n",
    "    return {\n",
    "        \"image\": image,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"target_ids\": target_ids\n",
    "    }\n",
    "\n",
    "# Apply preprocessing\n",
    "# print(preprocess(dataset[0]))\n",
    "dataset = dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhawzqJIB7J2"
   },
   "outputs": [],
   "source": [
    "# Bắt buộc để giữ tensor thay vì list!\n",
    "dataset.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1744964388474,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "KB6Kf-3u6FQs",
    "outputId": "3b4e2bdf-0e1b-47f5-9b93-d8a6438d4421"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 96, 96])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra xem data có đúng tensor ko\n",
    "example = dataset[0]\n",
    "print(type(example['image']))          # <class 'torch.Tensor'>\n",
    "print(example['image'].shape)          # torch.Size([3, 96, 96])\n",
    "print(example['input_ids'].shape)      # torch.Size([256])\n",
    "print(example['target_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0PmK6P63K_Y"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Đảm bảo chuyển về tensor đúng shape\n",
    "    imgs = torch.stack([torch.tensor(item['image']) if not isinstance(item['image'], torch.Tensor) else item['image'] for item in batch])\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    target_ids = torch.stack([item['target_ids'] for item in batch])\n",
    "    return imgs, input_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "La0FpbzP25iX"
   },
   "outputs": [],
   "source": [
    "n_embed, num_hiddens, num_heads, n_layer = 128, 512, 8, 8\n",
    "image_embed_dim = num_hiddens\n",
    "img_size = 96\n",
    "patch_size = 16\n",
    "num_blocks = 2\n",
    "\n",
    "n_layer, block_size, num_hiddens = 8, 32, 512\n",
    "\n",
    "# Initialize the model\n",
    "vlm = VisionLanguageModel(\n",
    "    n_embed=n_embed,\n",
    "    img_embed_dim=image_embed_dim,\n",
    "    vocab_size=tokenizer.vocab_size(),\n",
    "    n_layer=n_layer,\n",
    "    img_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    num_heads=num_heads,\n",
    "    num_blocks=num_blocks,\n",
    "    emb_dropout=0.1,\n",
    "    block_dropout=0.1,\n",
    ")\n",
    "device = torch.device('cpu')\n",
    "vlm.to(device)\n",
    "\n",
    "# Optimizer, chọn bộ phù hợp, chưa thử nhiều nên không bt bộ nào tốt\n",
    "# optimizer = torch.optim.AdamW(vlm.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.SGD(vlm.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9ulNDAvtI9r"
   },
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46403,
     "status": "ok",
     "timestamp": 1744964478616,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "j-FwZlCz26ew",
    "outputId": "a025d600-6ecc-4d37-ce3e-731a47c125a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 13/13 [00:04<00:00,  2.71it/s, loss=4.59]\n",
      "Epoch 34: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:25<00:00,  1.99s/it, loss=0.108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 8.9425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 13/13 [00:04<00:00,  3.15it/s, loss=0.192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 1.1465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 13/13 [00:04<00:00,  3.01it/s, loss=0.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 0.2088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 13/13 [00:04<00:00,  2.85it/s, loss=0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Avg Loss: 0.2247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 13/13 [00:04<00:00,  3.12it/s, loss=0.224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Loss: 0.2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 13/13 [00:05<00:00,  2.36it/s, loss=0.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Avg Loss: 0.2216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 13/13 [00:04<00:00,  3.14it/s, loss=0.214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Avg Loss: 0.2164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 13/13 [00:05<00:00,  2.25it/s, loss=0.206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Avg Loss: 0.2106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 13/13 [00:04<00:00,  2.70it/s, loss=0.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Avg Loss: 0.2043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 13/13 [00:04<00:00,  3.12it/s, loss=0.192]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Avg Loss: 0.1976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 - Avg Loss: 0.1069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:25<00:00,  1.93s/it, loss=0.111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 - Avg Loss: 0.1059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:24<00:00,  1.91s/it, loss=0.105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 - Avg Loss: 0.1042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:24<00:00,  1.91s/it, loss=0.104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 - Avg Loss: 0.1032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:24<00:00,  1.90s/it, loss=0.0992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Avg Loss: 0.1018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:24<00:00,  1.91s/it, loss=0.101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 - Avg Loss: 0.1006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40:  38%|█████████████████████████████████████████▏                                                                 | 5/13 [00:11<00:18,  2.31s/it, loss=0.103]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m _, loss = vlm(imgs, input_ids, targets=target_ids)\n\u001b[32m     13\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m total_loss += loss.item()\n\u001b[32m     17\u001b[39m pbar.set_postfix({\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m: loss.item()})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/train_vlm/.env/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/train_vlm/.env/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/train_vlm/.env/lib/python3.12/site-packages/torch/optim/sgd.py:121\u001b[39m, in \u001b[36mSGD.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    118\u001b[39m grads: List[Tensor] = []\n\u001b[32m    119\u001b[39m momentum_buffer_list: List[Optional[Tensor]] = []\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m has_sparse_grad = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m sgd(\n\u001b[32m    126\u001b[39m     params,\n\u001b[32m    127\u001b[39m     grads,\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     found_inf=\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfound_inf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    140\u001b[39m )\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[33m\"\u001b[39m\u001b[33mmomentum\u001b[39m\u001b[33m\"\u001b[39m] != \u001b[32m0\u001b[39m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/train_vlm/.env/lib/python3.12/site-packages/torch/optim/sgd.py:99\u001b[39m, in \u001b[36mSGD._init_group\u001b[39m\u001b[34m(self, group, params, grads, momentum_buffer_list)\u001b[39m\n\u001b[32m     97\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[33m\"\u001b[39m\u001b[33mmomentum\u001b[39m\u001b[33m\"\u001b[39m] != \u001b[32m0\u001b[39m:\n\u001b[32m     98\u001b[39m             state = \u001b[38;5;28mself\u001b[39m.state[p]\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m             momentum_buffer_list.append(\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmomentum_buffer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m has_sparse_grad\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "vlm.train()\n",
    "for epoch in range(1000):\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}\")\n",
    "    total_loss = 0\n",
    "    for imgs, input_ids, target_ids in pbar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        imgs = imgs.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        _, loss = vlm(imgs, input_ids, targets=target_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Avg Loss: {total_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1744964500753,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "Lr6G_VbLEm7L",
    "outputId": "8865130a-4299-4445-9989-55a6eea60061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(vlm.state_dict(), \"model_10.pth\")\n",
    "print(\"Model saved to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0x8bHVxvz8hB"
   },
   "source": [
    "## Eval\n",
    "### define model phải giống với lúc train, img_size=96, nếu đổi img_size phải đổi ở hàm def preprocess(example) và image_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1744964512054,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "wr6DhEADsp13",
    "outputId": "143ab83b-06f2-423a-c9e1-2529fdd8650b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "NjCgh2QRW5Iu"
   },
   "outputs": [],
   "source": [
    "tokenizer = spm.SentencePieceProcessor(model_file='./spm.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "dBizS78pV5yQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 30])\n",
      "torch.Size([1, 29])\n",
      "torch.Size([59])\n",
      "tensor([ 7581,   294,   458,   333, 38411,   263, 17974,   527,   321, 37727,\n",
      "          294,   336,   260, 29694,   736,   260, 10809, 12479,   716,   260,\n",
      "        51915,   802,   260,  7309, 10519,   275,   262,  1837,   260, 10519,\n",
      "        10519,   294,   736,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "n_embed, num_hiddens, num_heads, n_layer = 128, 512, 8, 8\n",
    "image_embed_dim = num_hiddens\n",
    "img_size = 96\n",
    "patch_size = 16\n",
    "num_blocks = 2\n",
    "\n",
    "n_layer, block_size, num_hiddens = 8, 32, 512\n",
    "\n",
    "# Initialize the model\n",
    "model = VisionLanguageModel(\n",
    "    n_embed=n_embed,\n",
    "    img_embed_dim=image_embed_dim,\n",
    "    vocab_size=tokenizer.vocab_size(),\n",
    "    n_layer=n_layer,\n",
    "    img_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    num_heads=num_heads,\n",
    "    num_blocks=num_blocks,\n",
    "    emb_dropout=0.1,\n",
    "    block_dropout=0.1,\n",
    ")\n",
    "# model.to(device)\n",
    "# checkpoint = torch.load(best_ckpt_path)\n",
    "model.load_state_dict(torch.load(\"./checkpoints/vlm_best.pt\")['model_state_dict'])\n",
    "model.eval()  # set to eval mode if you're going to do inference\n",
    "\n",
    "# Load image\n",
    "img_path = './image-1d100e9.jpg'  # 🔁 Replace with your actual image path\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# Preprocessing image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),  # make sure this matches ViT input\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "img_tensor = transform(image).unsqueeze(0)  # shape: [1, 3, 96, 96]\n",
    "\n",
    "# Move img_tensor to the same device as the model\n",
    "# img_tensor = img_tensor.to(device)\n",
    "\n",
    "# Prepare prompt and tokenize it\n",
    "prompt = \"Question: What do respiration and combustion give out\\nChoices:\\nA. Oxygen\\nB. Carbon dioxide\\nC. Nitrogen\\nD. Heat\\nAnswer with the letter.\"\n",
    "tokens = tokenizer.encode(prompt)\n",
    "\n",
    "# Convert tokens list to a PyTorch tensor\n",
    "tokens_tensor = torch.tensor(tokens).unsqueeze(0) # Add batch dimension and move to device\n",
    "\n",
    "# --- 4. Run inference ---\n",
    "with torch.no_grad():\n",
    "    output_tokens = model(\n",
    "        img_array=img_tensor,\n",
    "        idx=tokens_tensor,  # Pass tensor instead of list\n",
    "        # max_new_tokens=50\n",
    "    )\n",
    "    predictions = torch.argmax(output_tokens, dim=-1)  # shape: (batch_size, seq_len)   '\n",
    "    print(predictions.shape)\n",
    "    print(tokens_tensor.shape)\n",
    "    out = torch.cat([tokens_tensor, predictions], dim=1)\n",
    "    print(out.view(-1).shape)\n",
    "    print(out.view(-1))\n",
    "    # print(predictions)\n",
    "    # predictions = output_tokens\n",
    "\n",
    "    # Convert the tensor to a list and filter out pad_id\n",
    "    # output_tokens_list = [token.item() for token in predictions.flatten() if token.item() != tokenizer.pad_id()]\n",
    "    # print(tokenizer.decode(output_tokens_list))\n",
    "    # # print(output_tokens.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1744965288569,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "h9QzGJLfBnSg",
    "outputId": "91da19cb-8e3b-4f71-f934-9e701720edb5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Decode and handle special tokens ---\n",
    "# Convert tensor to list and decode\n",
    "output_tokens_list = output_tokens[0].cpu().numpy().tolist()\n",
    "\n",
    "print(output_tokens_list)\n",
    "\n",
    "# # Remove special tokens manually (if needed)\n",
    "# # For example, let's assume that 0 is the token for padding (common in many models)\n",
    "# # Modify the list to remove any special tokens, if necessary\n",
    "# output_tokens_list = [token for token in output_tokens_list if token != tokenizer.pad_id()]\n",
    "\n",
    "# # Now, decode the remaining tokens\n",
    "# output_text = tokenizer.decode(output_tokens_list)\n",
    "# print(\"Answer:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JA3bwtEDspaj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00dbc84d65d14669af49353af2fe81c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "017b1fd478bc493c9eae5aafc36ca0ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e19b6faf003d4d458b22ae755644dd4e",
       "IPY_MODEL_cc0b64d960f34603b9a191b1e55eddef",
       "IPY_MODEL_50b7ca0016e74b9b86b406a58f0eb37a"
      ],
      "layout": "IPY_MODEL_42a90f9929544cbb9b3540878f1ab9ff"
     }
    },
    "0f46dd8c14094ae19910136bb2ec19a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_202fee786bf74b198c48307ede3c55c6",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2a00aa99569f4bff8d506a6a282624f1",
      "value": 100
     }
    },
    "202fee786bf74b198c48307ede3c55c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a00aa99569f4bff8d506a6a282624f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3d8c3f01e1034f129e1e06e17bad751f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3df75f9d77464f19b10483b1b04468f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f2e003ba2e445658cf925566c53afe8",
      "placeholder": "​",
      "style": "IPY_MODEL_cdc95decd0694b5396f08e64aed715ae",
      "value": " 2434/2434 [00:35&lt;00:00, 72.35 examples/s]"
     }
    },
    "42a90f9929544cbb9b3540878f1ab9ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43ae542de31241efbf99b13b7b043359": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45d13ed6fef54059ae2c0c62f2e3481d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a85684c8ea1c41288d8e4a4d4b5c6b1f",
       "IPY_MODEL_0f46dd8c14094ae19910136bb2ec19a6",
       "IPY_MODEL_aacec83f466a43779ecc05ba9fada696"
      ],
      "layout": "IPY_MODEL_e60330e7e3704f119f666acb11fb93be"
     }
    },
    "45d1881f95724550bf82bc6bc85d19fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b38adec2ad1413c8b8dabcd41a3d9ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50b7ca0016e74b9b86b406a58f0eb37a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_704f403601be451db32201221aba1b78",
      "placeholder": "​",
      "style": "IPY_MODEL_fa7f6a094ac349f2b39e59675f1d6539",
      "value": " 2434/2434 [00:34&lt;00:00, 71.16 examples/s]"
     }
    },
    "5c46284f51b7416dacae0d9a67ee986c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_637ef79ba39b4eb18061ca44840569e4",
      "max": 2434,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b31bca7a1ee94fb780cf90a8ab4bfb52",
      "value": 2434
     }
    },
    "637ef79ba39b4eb18061ca44840569e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "704f403601be451db32201221aba1b78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7aab307a4aa44c1a9f439089284a2a30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f2e003ba2e445658cf925566c53afe8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97570c5dc77d4ebf92007397ddf01d64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6ced33ea5a04e5aae45deb1db6939df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a85684c8ea1c41288d8e4a4d4b5c6b1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee9e868961f64ab5843beb0aa6841216",
      "placeholder": "​",
      "style": "IPY_MODEL_3d8c3f01e1034f129e1e06e17bad751f",
      "value": "Map: 100%"
     }
    },
    "aacec83f466a43779ecc05ba9fada696": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbac3bf0c58a417996f62f2f2dd58c4b",
      "placeholder": "​",
      "style": "IPY_MODEL_7aab307a4aa44c1a9f439089284a2a30",
      "value": " 100/100 [00:20&lt;00:00,  1.61 examples/s]"
     }
    },
    "add9d266ded843afa4819be93e994c03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b31bca7a1ee94fb780cf90a8ab4bfb52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c49155ad5e0744d3b840c444f87fe4a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45d1881f95724550bf82bc6bc85d19fd",
      "placeholder": "​",
      "style": "IPY_MODEL_97570c5dc77d4ebf92007397ddf01d64",
      "value": "Filter: 100%"
     }
    },
    "cbac3bf0c58a417996f62f2f2dd58c4b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc0b64d960f34603b9a191b1e55eddef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b38adec2ad1413c8b8dabcd41a3d9ad",
      "max": 2434,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_add9d266ded843afa4819be93e994c03",
      "value": 2434
     }
    },
    "cdc95decd0694b5396f08e64aed715ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e19b6faf003d4d458b22ae755644dd4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43ae542de31241efbf99b13b7b043359",
      "placeholder": "​",
      "style": "IPY_MODEL_a6ced33ea5a04e5aae45deb1db6939df",
      "value": "Filter: 100%"
     }
    },
    "e60330e7e3704f119f666acb11fb93be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee9e868961f64ab5843beb0aa6841216": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f03af258ff3f44b198a34a4dfffb99ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c49155ad5e0744d3b840c444f87fe4a3",
       "IPY_MODEL_5c46284f51b7416dacae0d9a67ee986c",
       "IPY_MODEL_3df75f9d77464f19b10483b1b04468f0"
      ],
      "layout": "IPY_MODEL_00dbc84d65d14669af49353af2fe81c0"
     }
    },
    "fa7f6a094ac349f2b39e59675f1d6539": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
