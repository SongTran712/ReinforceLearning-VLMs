{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109969,
     "status": "ok",
     "timestamp": 1745302976473,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "2m_Ec3B3SZVV",
    "outputId": "38b808ae-be9c-47c3-8762-4db3453b38c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Downloading numpy-2.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.env/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in ./.env/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Collecting torch==2.6.0 (from torchvision)\n",
      "  Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Downloading pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.env/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (4.13.2)\n",
      "Collecting networkx (from torch==2.6.0->torchvision)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in ./.env/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch==2.6.0->torchvision)\n",
      "  Downloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: setuptools in ./.env/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (79.0.0)\n",
      "Collecting sympy==1.13.1 (from torch==2.6.0->torchvision)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch==2.6.0->torchvision)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.env/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.env/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.env/lib/python3.12/site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Downloading torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.6/766.6 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading numpy-2.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\n",
      "Downloading multidict-6.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "Downloading propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (245 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (349 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, sentencepiece, pytz, nvidia-cusparselt-cu12, mpmath, xxhash, tzdata, tqdm, sympy, pyarrow, propcache, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, fsspec, frozenlist, filelock, dill, aiohappyeyeballs, yarl, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, aiosignal, nvidia-cusolver-cu12, aiohttp, torch, torchvision, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 datasets-3.5.0 dill-0.3.8 filelock-3.18.0 frozenlist-1.6.0 fsspec-2024.12.0 huggingface-hub-0.30.2 mpmath-1.3.0 multidict-6.4.3 multiprocess-0.70.16 networkx-3.4.2 numpy-2.2.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pandas-2.2.3 pillow-11.2.1 propcache-0.3.1 pyarrow-19.0.1 pytz-2025.2 sentencepiece-0.2.0 sympy-1.13.1 torch-2.6.0 torchvision-0.21.0 tqdm-4.67.1 triton-3.2.0 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets torchvision sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 14132,
     "status": "ok",
     "timestamp": 1745302995197,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "7veYguFjspaV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/train_vlm/.env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import Tuple\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7J9Pd71spaW"
   },
   "source": [
    "## Converting Image to a Sequence of Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1745303000415,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "1-lrtKNFspaX"
   },
   "outputs": [],
   "source": [
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self, img_size: int = 96, patch_size: int = 16, hidden_dim: int = 512\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # Store the input image size, the patch size and hidden dimension\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Calculate the total number of patches\n",
    "        self.num_patches = (self.img_size // self.patch_size) ** 2\n",
    "\n",
    "        # Create a convolution to extract patch embeddings\n",
    "        # in_channels=3 asummes a 3-channel image (RGB)\n",
    "        # outp_channels=hidden_dim sets the number of output channels to match the hidden dimension\n",
    "        # kernel_size=patch_size and stride=patch_size ensuring each patch is embedded separately\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=self.hidden_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        # Extract patch embeddings from the input image\n",
    "        # Output shape: (batch_size, hidden_dim, (self.img_size // self.patch_size), (self.img_size // self.patch_size))\n",
    "        X = self.conv(X)\n",
    "\n",
    "        # Flatten the spatial dimensions (height and width) of the patch embeddings\n",
    "        # This step flattens the patch dimensions to a single dimension\n",
    "        # Output shape: (batch_size, hidden_dim, self.num_patches)\n",
    "        X = X.flatten(2)\n",
    "\n",
    "        # Transpose the dimensions to obtain the shape (batch_size, num_patches, hidden_dim)\n",
    "        # This step brings the num_patches dimension to the second position\n",
    "        # Output shape: (batch_size, self.num_patches, hidden_dim)\n",
    "        X = X.transpose(1, 2)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1745303004155,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "79pfUClUspaY",
    "outputId": "7fc1cabf-ad31-4f63-c65d-871940a9f024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image patches: torch.Size([1, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "B, C, H, W = 1, 3, 96, 96  # Batch size, Channels, Height, Width\n",
    "X = torch.randn(B, C, H, W)\n",
    "\n",
    "patch_size = 16\n",
    "hidden_dim = 512\n",
    "\n",
    "patch_embeddings = PatchEmbeddings(\n",
    "    img_size=H, patch_size=patch_size, hidden_dim=hidden_dim\n",
    ")\n",
    "patches = patch_embeddings(X)\n",
    "print(f\"Shape of image patches: {patches.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1745303005332,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "NTHJZgh0spaY",
    "outputId": "3facf4a2-5bd9-4d80-e6a2-bc33eb1645b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "num_patches = (H // patch_size) ** 2\n",
    "assert patches.shape == (B, num_patches, hidden_dim), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgQvJSuFspaZ"
   },
   "source": [
    "## Attention Mechanism\n",
    "Attention Mechanism across both the vision encoder and language decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xrdt5oNPspaZ"
   },
   "source": [
    "### The implementation of the Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1745303007120,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "_Zuq1aL5spaZ"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        head_size: int,\n",
    "        dropout: float = 0.1,\n",
    "        is_decoder: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear layer for Key projection\n",
    "        self.key = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "\n",
    "        # Linear layer for Query projection\n",
    "        self.query = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "\n",
    "        # Linear layer for Value projection\n",
    "        self.value = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "\n",
    "        # Dropout layer for regularization to prevent overfitting\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Flag indicating wheter the head is used as a decoder\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Get batch size (B), sequence length (T), and embedding dimension (C) from the input tensor\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Compute Key, Query, and Value projections\n",
    "        k = self.key(x)  # Shape: (B, T, head_size)\n",
    "        q = self.query(x)  # Shape: (B, T, head_size)\n",
    "        v = self.value(x)  # SHape: (B, T, head_size)\n",
    "\n",
    "        # Compute attention scores by taking the dot product of Query and Key\n",
    "        # and scaling by the square root of the embedding dimension\n",
    "        wei = q @ k.transpose(-2, -1) * (C**-0.5)  # Shape: (B, T, T)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # If this head is used in the decoder, apply causal mask to the attention scores\n",
    "            # to prevent attention to future positions\n",
    "            tril = torch.tril(torch.ones(T, T, dtype=torch.bool, device=x.device))\n",
    "            wei = wei.masked_fill(mask=tril == 0, value=float(\"-inf\"))\n",
    "\n",
    "        # Apply softmax to the attention scores to obtain attention probabilities\n",
    "        # Sum of probabilities for each row will be 1\n",
    "        wei = F.softmax(input=wei, dim=-1)  # Shape: (B, T, T)\n",
    "\n",
    "        # Apply Dropout to the attention probabilities for regularization\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Perform a weighted aggregation of values using the attention probabilities\n",
    "        out = wei @ v  # Shape: (B, T, head_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1745303013537,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "JzOfIfuwspaa",
    "outputId": "3a0facc5-d1a5-4656-ccd4-aa1cc436b3c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([1, 36, 16])\n"
     ]
    }
   ],
   "source": [
    "B, T, C = patches.shape  # Batch size, Sequence length, Embedding dimension\n",
    "head_size = 16  # Size of the attention head\n",
    "\n",
    "head = Head(n_embed=C, head_size=head_size)\n",
    "output = head(patches)\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1745303015496,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "eVTRkWHqspaa",
    "outputId": "3dbf5216-edc7-43d9-e45a-b490b21b0192"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, head_size), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmXF3DIhspaa"
   },
   "source": [
    "### The implementation of Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1745303017117,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "inh7qmxnspab"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.1,\n",
    "        is_decoder: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensure that the embedding dimension is divisible by the number of heads\n",
    "        assert n_embed % num_heads == 0, \"n_embed must be divisible by num_heads!\"\n",
    "\n",
    "        # Create a ModuleList of attention heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            modules=[\n",
    "                Head(\n",
    "                    n_embed=n_embed,\n",
    "                    head_size=n_embed // num_heads,\n",
    "                    dropout=dropout,\n",
    "                    is_decoder=is_decoder,\n",
    "                )\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Linear layer for projecting the concatenated head outputs\n",
    "        self.proj = nn.Linear(in_features=n_embed, out_features=n_embed)\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply each attention head to the input tensor\n",
    "        head_outputs = [\n",
    "            h(x) for h in self.heads\n",
    "        ]  # Shape: num_heads * (B, T, head_size)\n",
    "\n",
    "        # Concatenate the outputs from all heads along the last dimension\n",
    "        out = torch.cat(tensors=head_outputs, dim=-1)  # Shape: (B, T, m_embed)\n",
    "\n",
    "        # Apply the projection layer to the concatenated outputs\n",
    "        out = self.proj(out)  # Shape: (B, T, m_embed)\n",
    "\n",
    "        # Apply Dropout to the projected outputs for regularization\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1745303018449,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "4kRG1mj7spab"
   },
   "outputs": [],
   "source": [
    "num_heads = 2\n",
    "dropout = 0.1\n",
    "mha = MultiHeadAttention(n_embed=C, num_heads=num_heads, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1745303019397,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "7oaAkaZ7spab",
    "outputId": "edcb66de-0bac-48a3-a5dd-7fa3f498577d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([1, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "output = mha(patches)\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1745303020699,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "0I5qysIxspab",
    "outputId": "ae2793fc-fdca-424d-ae91-5d23e4855d6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, C), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ona5tht3spac"
   },
   "source": [
    "### The Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745303022869,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "1QPxO9cWspac"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_embed: int, dropout: float = 0.1, is_decoder: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the layers of the MLP\n",
    "        layers = [\n",
    "            # First linear layer that expands the input dimension from n_embed to 4 * n_embed\n",
    "            nn.Linear(in_features=n_embed, out_features=4 * n_embed),\n",
    "            # Activation function: ReLU if is_decoder is True, else GELU\n",
    "            nn.ReLU() if is_decoder else nn.GELU(),\n",
    "            # Second linear layer that projects the intermediate dimension back to n_embed\n",
    "            nn.Linear(in_features=4 * n_embed, out_features=n_embed),\n",
    "            # Dropout layer for regularization\n",
    "            nn.Dropout(p=dropout),\n",
    "        ]\n",
    "\n",
    "        # Create the MLP as a sequence of layers\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Pass the input through the MLP layers\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1745303024553,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "BK7FPIPyspac"
   },
   "outputs": [],
   "source": [
    "dropout = 0.1\n",
    "mlp = MLP(n_embed=C, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1745303025568,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "AF3WnIsospac",
    "outputId": "312131d6-dd21-4e99-8d6f-2562502a136a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([1, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "output = mlp(output)  # Previous output of the Multihead Attention\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1745303026688,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "0Gj0NrF8spad",
    "outputId": "31f9ad8f-1f94-4d50-aed2-8cce238d2d72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, C), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6888PMDpspad"
   },
   "source": [
    "### Transformer Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1745303028511,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "jLy1uQa2spad"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.1,\n",
    "        is_decoder: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Layer normalization for the input to the attention layer\n",
    "        self.ln1 = nn.LayerNorm(normalized_shape=n_embed)\n",
    "\n",
    "        # Multi-head attention module\n",
    "        self.mhattn = MultiHeadAttention(\n",
    "            n_embed=n_embed, num_heads=num_heads, dropout=dropout, is_decoder=is_decoder\n",
    "        )\n",
    "\n",
    "        # Layer normalization for the input to the FFN\n",
    "        self.ln2 = nn.LayerNorm(normalized_shape=n_embed)\n",
    "\n",
    "        # Feed-forward neural network (FFN)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_features=n_embed, out_features=4 * n_embed),\n",
    "            nn.GELU(),  # Activation function\n",
    "            nn.Linear(\n",
    "                in_features=4 * n_embed, out_features=n_embed\n",
    "            ),  # Projection back to the original dimension\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Saving the input for residual connection\n",
    "        original_x = x\n",
    "\n",
    "        # Apply layer normalization to the input\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        # Apply multi-head attention\n",
    "        mhattn_output = self.mhattn(x)\n",
    "\n",
    "        # Add the residual connection (original input) to the attention output\n",
    "        x = original_x + mhattn_output\n",
    "\n",
    "        # Apply later normalization to the input to the FFN\n",
    "        x = self.ln2(x)\n",
    "\n",
    "        # Apply the FFN\n",
    "        ffn_output = self.ffn(x)\n",
    "\n",
    "        # Apply the residual connection (input to the FFN) to the FFN output\n",
    "        x = x + ffn_output\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1745303029987,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "trl5mtK5spad"
   },
   "outputs": [],
   "source": [
    "num_heads = 2\n",
    "dropout = 0.1\n",
    "block = Block(n_embed=C, num_heads=num_heads, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745303031188,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "xZMaSLSCspad",
    "outputId": "072fba58-82ed-4adb-dc7d-0eea71693846"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([1, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "output = block(patches)\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1745303032249,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "POugdyswspad",
    "outputId": "c467b86a-b268-4fdb-b1ac-08cd975e6327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, C), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h31vzsbQspae"
   },
   "source": [
    "## Vision Encoder - Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DNEtMj-spae"
   },
   "source": [
    "Combining patchification logic and attention block in to ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745303034064,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "YGX1s-qCspae"
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int,\n",
    "        patch_size: int,\n",
    "        num_hiddens: int,\n",
    "        num_heads: int,\n",
    "        num_blocks: int,\n",
    "        emb_dropout: float,\n",
    "        block_dropout: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding layer to convert the input image into patches\n",
    "        self.patch_embedding = PatchEmbeddings(\n",
    "            img_size=img_size, patch_size=patch_size, hidden_dim=num_hiddens\n",
    "        )\n",
    "\n",
    "        # Learnable classification token\n",
    "        self.cls_token = nn.Parameter(data=torch.zeros(size=(1, 1, num_hiddens)))\n",
    "\n",
    "        # Calculate the number of patches\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Learnable position embedding\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            data=torch.randn(size=(1, num_patches + 1, num_hiddens))\n",
    "        )\n",
    "\n",
    "        # Dropout layer for the embeddings\n",
    "        self.dropout = nn.Dropout(p=emb_dropout)\n",
    "\n",
    "        # Stack of transformer blocks\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    n_embed=num_hiddens,\n",
    "                    num_heads=num_heads,\n",
    "                    dropout=block_dropout,\n",
    "                    is_decoder=False,\n",
    "                )\n",
    "                for _ in range(num_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Layer normalization for the final representation\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=num_hiddens)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        # Convert the input image into patch embeddings\n",
    "        x = self.patch_embedding(X)  # Shape: (B, num_patches, num_hiddens)\n",
    "\n",
    "        # Expand the classification token to match the batch size\n",
    "        cls_tokens = self.cls_token.expand(\n",
    "            x.shape[0], -1, -1\n",
    "        )  # Shape: (B, 1, num_hiddens)\n",
    "\n",
    "        # Concatenate the classification token with the patch embeddings\n",
    "        x = torch.cat(\n",
    "            tensors=(cls_tokens, x), dim=1\n",
    "        )  # Shape: (B, num_patches + 1, num_hiddens)\n",
    "\n",
    "        # Add the position embedding to the patch embeddings\n",
    "        x += self.pos_embedding  # Shape: (B, num_patches + 1, num_hiddens)\n",
    "\n",
    "        # Apply dropout to the embeddings\n",
    "        x = self.dropout(x)  # Shape: (B, num_patches + 1, num_hiddens)\n",
    "\n",
    "        # Pass the embeddings through the transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)  # Shape: (B, num_patches + 1, num_hiddens)\n",
    "\n",
    "        # Apply layer normalization to the `[CLS]` token's final representation\n",
    "        x = self.layer_norm(x[:, 0])  # Shape: (B, num_hiddens)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745303036549,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "cYwJwvrjspae"
   },
   "outputs": [],
   "source": [
    "B, C, H, W = 2, 3, 96, 96  # Batch size, Channels, Height, Width\n",
    "X = torch.randn(B, C, H, W)\n",
    "vit = ViT(\n",
    "    img_size=H,\n",
    "    patch_size=16,\n",
    "    num_hiddens=64,\n",
    "    num_heads=2,\n",
    "    num_blocks=2,\n",
    "    emb_dropout=0.1,\n",
    "    block_dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1745303037969,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "GVwEBk8Sspae",
    "outputId": "0288437a-38f5-461c-df7d-d82273687366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "output = vit(X)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1745303039213,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "OCrJkhG2spaf",
    "outputId": "b48d4ff4-e87d-4368-d86b-ccdd8ab5e2f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, 64), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1teH1MPspaf"
   },
   "source": [
    "## Vision-Language Projection Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EuAJV2rspaf"
   },
   "source": [
    "Unfortunatelly, we can't directly concatenate ViT output to the text embeddings. <br>\n",
    "We need to project this from dimensionality of image embeddings from the vision transformer to the dimensionality of text embeddings.\n",
    "\n",
    "Why MLP for this part? If you want to train VLM with low resources you can do so by keeping both the pretrained vision encoder and language decoder frozen during the VLM training. Therefore, allocating more parameters to the connection module could enhance the overall VLM's ability to generalize and help in the downstream instruction-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1745303043943,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "fsAv9P6aspaf"
   },
   "outputs": [],
   "source": [
    "class MultiModalProjector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        img_embed_dim: int,\n",
    "        dropout: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the projection network\n",
    "        self.net = nn.Sequential(\n",
    "            # Linear layer to expand the image embedding dimension\n",
    "            nn.Linear(in_features=img_embed_dim, out_features=4 * img_embed_dim),\n",
    "            # GELU activation function\n",
    "            nn.GELU(),\n",
    "            # Linear layer to project the expanded image embeddings to the text embedding dimension\n",
    "            nn.Linear(in_features=4 * img_embed_dim, out_features=n_embed),\n",
    "            # Dropout layer for regularization\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Pass the input through the projection network\n",
    "        x = self.net(x)  # Shape: (B, img_embed_dim) --> (B, n_embed)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1745303045845,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "l7ZYNX_sspaf"
   },
   "outputs": [],
   "source": [
    "B, n_embed, img_embed_dim = 2, 64, 128\n",
    "X = torch.randn(size=(B, img_embed_dim))\n",
    "\n",
    "projector = MultiModalProjector(\n",
    "    n_embed=n_embed, img_embed_dim=img_embed_dim, dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745303046992,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "TR2UZ-Kgspaf",
    "outputId": "387d1e35-1629-4f3c-91ab-ed7485ea339c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "output = projector(X)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1745303048116,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "TYJa4bTqspaf",
    "outputId": "772a624c-8048-4b2a-d870-13f27f2ef4a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, n_embed), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYlkygxcspag"
   },
   "source": [
    "## Building the Decoder Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unGnK01Yspag"
   },
   "source": [
    "Only thing that deviates from origianl implementation is that here projection module is integrated into decoder model class. <br>\n",
    "In contrary, when using pretrained models with HuggingFace (or any other library), you can directly feed embeddings as input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745303049998,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "UMcEL2qvspag"
   },
   "outputs": [],
   "source": [
    "class DecoderLanguageModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        img_embed_dim: int,\n",
    "        vocab_size: int,\n",
    "        num_heads: int,\n",
    "        n_layer: int,\n",
    "        use_images: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_images = use_images\n",
    "\n",
    "        # Token embedding table\n",
    "        self.token_embedding_table = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=n_embed\n",
    "        )\n",
    "\n",
    "        # Position embedding table\n",
    "        self.position_embedding_table = nn.Embedding(\n",
    "            num_embeddings=1000, embedding_dim=n_embed\n",
    "        )\n",
    "\n",
    "        if use_images:\n",
    "            # Image projection layer to align image embeddings with text embeddings\n",
    "            self.image_projection = MultiModalProjector(\n",
    "                n_embed=n_embed, img_embed_dim=img_embed_dim\n",
    "            )\n",
    "\n",
    "        # Stack of transformer decoder blocks\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                Block(n_embed=n_embed, num_heads=num_heads, is_decoder=True)\n",
    "                for _ in range(n_layer)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(normalized_shape=n_embed)\n",
    "\n",
    "        # Language modeling head\n",
    "        self.lm_head = nn.Linear(in_features=n_embed, out_features=vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        idx: torch.Tensor,\n",
    "        img_embeds: torch.Tensor = None,\n",
    "        targets: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # Get token embeddings from the input indices\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "\n",
    "        if self.use_images:\n",
    "            # Project and concatenate image embeddings with token embeddings\n",
    "            img_emb = self.image_projection(img_embeds).unsqueeze(1)\n",
    "            tok_emb = torch.cat([img_emb, tok_emb], dim=1)\n",
    "\n",
    "        # Get position embeddings\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            torch.arange(tok_emb.size(1), device=idx.device)\n",
    "        )\n",
    "\n",
    "        # Add position embeddings to token embeddings\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # Pass through the transformer decoder blocks\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # Apply final layer normalization\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        # Get the logits from the language modeling head\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            if self.use_images and img_embeds is not None:\n",
    "                # Prepare targets by concatenating a dummy target for the image embedding\n",
    "                batch_size = idx.size(0)\n",
    "                targets = torch.cat(\n",
    "                    [\n",
    "                        torch.full(\n",
    "                            (batch_size, 1), -100, dtype=torch.long, device=idx.device\n",
    "                        ),\n",
    "                        targets,\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "            # Compute the cross-entropy loss\n",
    "            loss = F.cross_entropy(\n",
    "                input=logits.view(-1, logits.size(-1)),\n",
    "                target=targets.view(-1),\n",
    "                ignore_index=-100,\n",
    "            )\n",
    "            return logits, loss\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate(\n",
    "        self, idx: torch.Tensor, img_embeds: torch.Tensor, max_new_tokens: int\n",
    "    ) -> torch.Tensor:\n",
    "        # Get the batch size and sequence length\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Initialize the generated sequence with the input indices\n",
    "        generated = idx\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "\n",
    "        if self.use_images and img_embeds is not None:\n",
    "            # Project and concatenate image embeddings with token embeddings\n",
    "            img_emb = self.image_projection(img_embeds).unsqueeze(1)\n",
    "            current_output = torch.cat([img_emb, tok_emb], dim=1)\n",
    "        else:\n",
    "            current_output = tok_emb\n",
    "\n",
    "        # Generate new tokens iteratevely\n",
    "        for i in range(max_new_tokens):\n",
    "            # Get the current sequence length\n",
    "            T_current = current_output.shape[1]\n",
    "\n",
    "            # Get position embeddings for the current sequence length\n",
    "            current_pos_emb = self.position_embedding_table(\n",
    "                torch.arange(T_current, device=idx.device)\n",
    "            ).unsqueeze(0)\n",
    "\n",
    "            # Add position embeddings to the current output\n",
    "            current_output += current_pos_emb\n",
    "\n",
    "            # Pass through the transformer decoder blocks\n",
    "            for block in self.blocks:\n",
    "                current_output = block(current_output)\n",
    "\n",
    "            # Get the logits for the last token\n",
    "            logits = self.lm_head(current_output[:, -1, :])\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Sample the next token based on the probability\n",
    "            idx_next = torch.multinomial(input=probs, num_samples=1)\n",
    "\n",
    "            # Concatenate the generated token to the generated sequence\n",
    "            generated = torch.cat([generated, idx_next], dim=1)\n",
    "\n",
    "            # Get the embeddings for the generated token\n",
    "            idx_next_emb = self.token_embedding_table(idx_next)\n",
    "\n",
    "            # Concatenate the generated token embeddings to the current output\n",
    "            current_output = torch.cat([current_output, idx_next_emb], dim=1)\n",
    "\n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1745303051615,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "DziYVmmNxU1b",
    "outputId": "945ac5bc-e779-47aa-d68e-177e4e157b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4043, -0.2360,  0.7757, -0.6890,  0.5713,  0.7772,  0.7611,\n",
      "           0.7728, -1.7342,  2.3012],\n",
      "         [-1.2186, -0.5990,  0.6929,  0.2358, -1.5885, -0.0234,  0.4622,\n",
      "           0.2673,  0.6569,  0.1141],\n",
      "         [-1.4403, -0.2727,  1.9735, -0.0568,  0.6660, -0.4947, -0.5999,\n",
      "           0.4363, -0.3913, -0.5853],\n",
      "         [-1.3703,  0.4677,  0.9049, -0.5474, -0.6246, -1.3839,  1.9559,\n",
      "           0.5793,  1.0454, -0.0923],\n",
      "         [-0.4327, -1.2240,  0.0360,  1.7603,  0.5940, -0.4259, -0.0973,\n",
      "           0.0160, -0.9349, -1.7356]],\n",
      "\n",
      "        [[-0.9396,  1.5042,  1.2620,  0.5159, -0.4417,  1.2174, -0.5435,\n",
      "          -1.5183, -0.3332, -0.9561],\n",
      "         [ 1.3663, -1.3814, -0.7335,  2.0053,  0.2647, -1.0977, -0.1515,\n",
      "           1.3244, -0.1602,  1.2682],\n",
      "         [-0.2830,  0.2802,  1.0564,  0.5530, -0.2721, -0.9782, -0.2651,\n",
      "           0.8758, -0.8674, -1.5603],\n",
      "         [ 1.6397,  1.5049, -2.1373,  0.3688, -1.2621,  0.1268, -1.0480,\n",
      "           0.9555,  0.1263, -0.3048],\n",
      "         [-0.5043,  0.6646,  0.8138,  0.7129,  0.7423,  1.2015, -1.9114,\n",
      "           1.3942,  1.1584, -0.1554]]])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10])\n",
      "Loss: 2.6073195934295654\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simulated inputs\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "vocab_size = 10\n",
    "\n",
    "# Fake target labels (batch_size x seq_len)\n",
    "targets = torch.tensor([\n",
    "    [1, 2, 3, 4],\n",
    "    [2, 3, 4, 5]\n",
    "], dtype=torch.long)  # shape: [2, 4]\n",
    "\n",
    "# Dummy index tensor for alignment (same batch_size)\n",
    "idx = torch.arange(batch_size)\n",
    "\n",
    "# Simulated logits output from a model (batch_size x seq_len+1 x vocab_size)\n",
    "# We assume image embeddings are used, so seq_len + 1\n",
    "logits = torch.randn(batch_size, seq_len + 1, vocab_size)  # shape: [2, 5, 10]\n",
    "print(logits)\n",
    "\n",
    "# Add dummy target token for image embedding (-100 will be ignored in loss)\n",
    "targets = torch.cat(\n",
    "    [\n",
    "        torch.full(\n",
    "            (batch_size, 1), -100, dtype=torch.long, device=targets.device\n",
    "        ),\n",
    "        targets\n",
    "    ],\n",
    "    dim=1\n",
    ")  # shape: [2, 5]\n",
    "\n",
    "print(logits.view(-1, vocab_size).shape)\n",
    "print(targets.view(-1).shape)\n",
    "\n",
    "# Confirm shapes match\n",
    "assert logits.shape[:2] == targets.shape, f\"Shape mismatch: {logits.shape[:2]} vs {targets.shape}\"\n",
    "\n",
    "# Compute cross-entropy loss\n",
    "loss = F.cross_entropy(\n",
    "    input=logits.view(-1, vocab_size),  # shape: [2*5, 10]\n",
    "    target=targets.view(-1),            # shape: [2*5]\n",
    "    ignore_index=-100\n",
    ")\n",
    "\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1745303053121,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "eMfZ5bfSspag",
    "outputId": "85228429-d569-483d-c5f9-2ab36d6e151c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB_uIrRsspag"
   },
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1786,
     "status": "ok",
     "timestamp": 1745303059188,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "dOeSBdLCspag",
    "outputId": "438d6160-7564-4c88-f3dc-b2a576240036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([10, 51, 1000]), Loss: 7.057732582092285\n",
      "Generated sequence shape: torch.Size([10, 70])\n"
     ]
    }
   ],
   "source": [
    "n_embed, img_embed_dim, vocab_size, num_heads, n_layer = 128, 256, 1000, 8, 6\n",
    "# `n_layer` is used to represent number of decoder transformer blocks and num_blocks for the vision encoder to avoid confusion\n",
    "model = DecoderLanguageModel(\n",
    "    n_embed=n_embed,\n",
    "    img_embed_dim=img_embed_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    num_heads=num_heads,\n",
    "    n_layer=n_layer,\n",
    "    use_images=True,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Dummy input\n",
    "B, T = 10, 50\n",
    "idx = torch.randint(low=0, high=vocab_size, size=(B, T)).to(device)\n",
    "image_embeds = torch.randn(B, 256).to(device)  # Assume img_embed_dim is 256\n",
    "\n",
    "targets = torch.randint(0, vocab_size, (B, T)).to(\n",
    "    device\n",
    ")  # Only if you want to compute loss\n",
    "\n",
    "# Test forward pass\n",
    "# Check if you need to calculate loss by providing targets\n",
    "if targets is not None:\n",
    "    logits, loss = model(idx, image_embeds, targets)\n",
    "    print(f\"Logits shape: {logits.shape}, Loss: {loss}\")\n",
    "else:\n",
    "    logits = model(idx, image_embeds)  # Call without targets\n",
    "    print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "# Test generation\n",
    "generated = model.generate(idx, image_embeds, max_new_tokens=20)\n",
    "print(f\"Generated sequence shape: {generated.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93X7Cpv3spah"
   },
   "source": [
    "## Putting everything together: Simple Vision Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1745303062504,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "629X82iGspah"
   },
   "outputs": [],
   "source": [
    "class VisionLanguageModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        img_embed_dim: int,\n",
    "        vocab_size: int,\n",
    "        n_layer: int,\n",
    "        img_size: int,\n",
    "        patch_size: int,\n",
    "        num_heads: int,\n",
    "        num_blocks: int,\n",
    "        emb_dropout: float,\n",
    "        block_dropout: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Set num_hiddens equal to img_embed_dim\n",
    "        num_hiddens = img_embed_dim\n",
    "\n",
    "        # Assert that num_hiddens is divisible by num_heads\n",
    "        assert num_hiddens % num_heads == 0, ValueError(\n",
    "            \"num_hiddens must be divisible by num_heads!\"\n",
    "        )\n",
    "\n",
    "        # Initialize the Vision Transformer (ViT) encoder\n",
    "        self.vision_encoder = ViT(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            num_hiddens=num_hiddens,\n",
    "            num_heads=num_heads,\n",
    "            num_blocks=num_blocks,\n",
    "            emb_dropout=emb_dropout,\n",
    "            block_dropout=block_dropout,\n",
    "        )\n",
    "\n",
    "        # Initialize the Language Model Decoder (DecoderLanguageModel)\n",
    "        self.decoder = DecoderLanguageModel(\n",
    "            n_embed=n_embed,\n",
    "            img_embed_dim=img_embed_dim,\n",
    "            vocab_size=vocab_size,\n",
    "            num_heads=num_heads,\n",
    "            n_layer=n_layer,\n",
    "            use_images=True,\n",
    "        )\n",
    "\n",
    "    def _check_image_embeddings(self, image_embeds: torch.Tensor) -> None:\n",
    "        \"\"\"Chek if image embeddings are valid.\"\"\"\n",
    "        if image_embeds.nelement() == 0 or image_embeds.shape[1] == 0:\n",
    "            raise ValueError(\n",
    "                \"Something is wrong with the ViT model. It's returning an empty tensor or the embedding dimension is empty.\"\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self, img_array: torch.Tensor, idx: torch.Tensor, targets: torch.Tensor = None\n",
    "    ) -> torch.Tensor | Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Get the image embeddings from the Vision Encoder\n",
    "        image_embeds = self.vision_encoder(img_array)\n",
    "\n",
    "        # Check if image embeddings are valid\n",
    "        self._check_image_embeddings(image_embeds)\n",
    "\n",
    "        if targets is not None:\n",
    "            # If targets are provided, compute the logits and loss\n",
    "            logits, loss = self.decoder(idx, image_embeds, targets)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            # If targets are not provided, compute only the logits\n",
    "            logits = self.decoder(idx, image_embeds)\n",
    "            return logits\n",
    "\n",
    "    def generate(\n",
    "        self, img_array: torch.Tensor, idx: torch.Tensor, max_new_tokens: int\n",
    "    ) -> torch.Tensor:\n",
    "        # Get the image embeddings from the Vision Encoder\n",
    "        image_embeds = self.vision_encoder(img_array)\n",
    "\n",
    "        # Check if image embeddings are valid\n",
    "        self._check_image_embeddings(image_embeds)\n",
    "\n",
    "        # Generate new tokens using the Language Model Decoder\n",
    "        generated_tokens = self.decoder.generate(\n",
    "            idx=idx, img_embeds=image_embeds, max_new_tokens=max_new_tokens\n",
    "        )\n",
    "        return generated_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oK3RP2Sspai"
   },
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 520,
     "status": "ok",
     "timestamp": 1745303065738,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "olxCWjIbspai",
    "outputId": "670ca330-1ee2-49be-89f3-5abe42e7516c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from initialization forward pass: tensor([[[ 0.1523,  0.2643, -0.7298,  ...,  0.9584,  0.7303, -0.4125],\n",
      "         [ 0.5711,  0.1386,  0.6068,  ...,  0.2104, -0.2524, -0.1402],\n",
      "         [ 0.2705, -0.0751,  0.0375,  ..., -0.5754,  0.3984, -0.7458],\n",
      "         ...,\n",
      "         [-0.5594,  0.6825, -1.0122,  ...,  0.5349, -0.2477, -0.0164],\n",
      "         [-0.0354,  0.6391,  0.2511,  ...,  1.8003, -0.0955, -0.3355],\n",
      "         [ 0.3329,  0.3013,  0.5336,  ...,  0.2116, -0.6363,  0.1007]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n_embed, num_hiddens, vocab_size, num_heads, n_layer = 128, 512, 1000, 8, 8\n",
    "image_embed_dim = num_hiddens\n",
    "img_size = 96\n",
    "patch_size = 16\n",
    "num_blocks = 2\n",
    "\n",
    "n_layer, block_size, num_hiddens = 8, 32, 512\n",
    "\n",
    "# Initialize the model\n",
    "model = VisionLanguageModel(\n",
    "    n_embed=n_embed,\n",
    "    img_embed_dim=image_embed_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    n_layer=n_layer,\n",
    "    img_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    num_heads=num_heads,\n",
    "    num_blocks=num_blocks,\n",
    "    emb_dropout=0.1,\n",
    "    block_dropout=0.1,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Create dummy data with correct dimensions\n",
    "dummy_img = torch.randn(1, 3, img_size, img_size).to(\n",
    "    device\n",
    ")  # Correct shape for image input\n",
    "dummy_idx = torch.randint(0, vocab_size, (1, block_size)).to(\n",
    "    device\n",
    ")  # Correct shape for text input\n",
    "\n",
    "# Forward pass to initialize all parameters\n",
    "try:\n",
    "    output = model(dummy_img, dummy_idx)  # Output for debugging\n",
    "    print(\"Output from initialization forward pass:\", output)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Runtime Error during forward pass: {str(e)}\")\n",
    "    print(\"Check layer configurations and input shapes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745303068888,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "A5hDtsN9spai",
    "outputId": "fe07c6d1-2772-48d5-c7d2-bd880d4ecdbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 33, 1000])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ph6-Ca37z5ai"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1745303159482,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "gIY0SvARSxKh"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = spm.SentencePieceProcessor(model_file='spm.model')\n",
    "max_len = 256  # giới hạn token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1745303160893,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "YYllzhImS9BU"
   },
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "017b1fd478bc493c9eae5aafc36ca0ed",
      "e19b6faf003d4d458b22ae755644dd4e",
      "cc0b64d960f34603b9a191b1e55eddef",
      "50b7ca0016e74b9b86b406a58f0eb37a",
      "42a90f9929544cbb9b3540878f1ab9ff",
      "43ae542de31241efbf99b13b7b043359",
      "a6ced33ea5a04e5aae45deb1db6939df",
      "4b38adec2ad1413c8b8dabcd41a3d9ad",
      "add9d266ded843afa4819be93e994c03",
      "704f403601be451db32201221aba1b78",
      "fa7f6a094ac349f2b39e59675f1d6539"
     ]
    },
    "executionInfo": {
     "elapsed": 35730,
     "status": "ok",
     "timestamp": 1745303222813,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "-zyZX5q_S-DY",
    "outputId": "1623872e-3a22-4215-fa1b-b04312b5387e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017b1fd478bc493c9eae5aafc36ca0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2434 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"HuggingFaceM4/the_cauldron\", \"ai2d\", split=\"train\")\n",
    "\n",
    "# Lọc những entry có ảnh và text hợp lệ\n",
    "dataset = dataset.filter(lambda x: x[\"images\"] and x[\"texts\"] and \"user\" in x[\"texts\"][0])\n",
    "\n",
    "# # Lấy 100 sample đầu tiên, lấy full dataset thì bỏ qua dòng này\n",
    "# dataset = dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "f03af258ff3f44b198a34a4dfffb99ea",
      "c49155ad5e0744d3b840c444f87fe4a3",
      "5c46284f51b7416dacae0d9a67ee986c",
      "3df75f9d77464f19b10483b1b04468f0",
      "00dbc84d65d14669af49353af2fe81c0",
      "45d1881f95724550bf82bc6bc85d19fd",
      "97570c5dc77d4ebf92007397ddf01d64",
      "637ef79ba39b4eb18061ca44840569e4",
      "b31bca7a1ee94fb780cf90a8ab4bfb52",
      "7f2e003ba2e445658cf925566c53afe8",
      "cdc95decd0694b5396f08e64aed715ae",
      "45d13ed6fef54059ae2c0c62f2e3481d",
      "a85684c8ea1c41288d8e4a4d4b5c6b1f",
      "0f46dd8c14094ae19910136bb2ec19a6",
      "aacec83f466a43779ecc05ba9fada696",
      "e60330e7e3704f119f666acb11fb93be",
      "ee9e868961f64ab5843beb0aa6841216",
      "3d8c3f01e1034f129e1e06e17bad751f",
      "202fee786bf74b198c48307ede3c55c6",
      "2a00aa99569f4bff8d506a6a282624f1",
      "cbac3bf0c58a417996f62f2f2dd58c4b",
      "7aab307a4aa44c1a9f439089284a2a30"
     ]
    },
    "executionInfo": {
     "elapsed": 57214,
     "status": "ok",
     "timestamp": 1744964343594,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "00mLCaeIuqXb",
    "outputId": "2812104e-868f-44f2-df87-d04acb19c1f0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03af258ff3f44b198a34a4dfffb99ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2434 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d13ed6fef54059ae2c0c62f2e3481d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocessing function\n",
    "def preprocess(example):\n",
    "    img_data = example[\"images\"][0]\n",
    "\n",
    "    # Đảm bảo ảnh là PIL.Image\n",
    "    if isinstance(img_data, Image.Image):\n",
    "        img = img_data.convert(\"RGB\")\n",
    "    elif isinstance(img_data, np.ndarray):\n",
    "        img = Image.fromarray(img_data).convert(\"RGB\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported image format: {type(img_data)}\")\n",
    "\n",
    "    # Transform để ra Tensor\n",
    "    image = image_transform(img)  # Tensor (3, 224, 224)\n",
    "\n",
    "    # Tokenize prompt và target\n",
    "    prompt = example[\"texts\"][0][\"user\"]\n",
    "    target = example[\"texts\"][0].get(\"assistant\", \"\")\n",
    "\n",
    "    full_input = prompt + \"\\n\" + target if target else prompt\n",
    "\n",
    "    pad_id = tokenizer.pad_id() if tokenizer.pad_id() >= 0 else 0\n",
    "    tokens = tokenizer.encode(full_input)\n",
    "    tokens = tokens[:max_len]\n",
    "    tokens += [pad_id] * (max_len - len(tokens))\n",
    "    input_ids = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "    # Tokenize target\n",
    "    if target:\n",
    "        target_tokens = tokenizer.encode(target)\n",
    "        target_tokens = target_tokens[:max_len]\n",
    "        target_tokens += [pad_id] * (max_len - len(target_tokens))\n",
    "        target_ids = torch.tensor(target_tokens, dtype=torch.long)\n",
    "    else:\n",
    "        target_ids = torch.full_like(input_ids, fill_value=pad_id)\n",
    "\n",
    "    return {\n",
    "        \"image\": image,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"target_ids\": target_ids\n",
    "    }\n",
    "\n",
    "# Apply preprocessing\n",
    "dataset = dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhawzqJIB7J2"
   },
   "outputs": [],
   "source": [
    "# Bắt buộc để giữ tensor thay vì list!\n",
    "dataset.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1744964388474,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "KB6Kf-3u6FQs",
    "outputId": "3b4e2bdf-0e1b-47f5-9b93-d8a6438d4421"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 96, 96])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra xem data có đúng tensor ko\n",
    "example = dataset[0]\n",
    "print(type(example['image']))          # <class 'torch.Tensor'>\n",
    "print(example['image'].shape)          # torch.Size([3, 96, 96])\n",
    "print(example['input_ids'].shape)      # torch.Size([256])\n",
    "print(example['target_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0PmK6P63K_Y"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Đảm bảo chuyển về tensor đúng shape\n",
    "    imgs = torch.stack([torch.tensor(item['image']) if not isinstance(item['image'], torch.Tensor) else item['image'] for item in batch])\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    target_ids = torch.stack([item['target_ids'] for item in batch])\n",
    "    return imgs, input_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "La0FpbzP25iX"
   },
   "outputs": [],
   "source": [
    "n_embed, num_hiddens, num_heads, n_layer = 128, 512, 8, 8\n",
    "image_embed_dim = num_hiddens\n",
    "img_size = 96\n",
    "patch_size = 16\n",
    "num_blocks = 2\n",
    "\n",
    "n_layer, block_size, num_hiddens = 8, 32, 512\n",
    "\n",
    "# Initialize the model\n",
    "vlm = VisionLanguageModel(\n",
    "    n_embed=n_embed,\n",
    "    img_embed_dim=image_embed_dim,\n",
    "    vocab_size=tokenizer.vocab_size(),\n",
    "    n_layer=n_layer,\n",
    "    img_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    num_heads=num_heads,\n",
    "    num_blocks=num_blocks,\n",
    "    emb_dropout=0.1,\n",
    "    block_dropout=0.1,\n",
    ")\n",
    "device = torch.device('cuda')\n",
    "vlm.to(device)\n",
    "\n",
    "# Optimizer, chọn bộ phù hợp, chưa thử nhiều nên không bt bộ nào tốt\n",
    "# optimizer = torch.optim.AdamW(vlm.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.SGD(vlm.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9ulNDAvtI9r"
   },
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46403,
     "status": "ok",
     "timestamp": 1744964478616,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "j-FwZlCz26ew",
    "outputId": "a025d600-6ecc-4d37-ce3e-731a47c125a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 13/13 [00:04<00:00,  2.71it/s, loss=4.59]\n",
      "Epoch 34: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:25<00:00,  1.99s/it, loss=0.108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 8.9425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 13/13 [00:04<00:00,  3.15it/s, loss=0.192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 1.1465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 13/13 [00:04<00:00,  3.01it/s, loss=0.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 0.2088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 13/13 [00:04<00:00,  2.85it/s, loss=0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Avg Loss: 0.2247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 13/13 [00:04<00:00,  3.12it/s, loss=0.224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Loss: 0.2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 13/13 [00:05<00:00,  2.36it/s, loss=0.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Avg Loss: 0.2216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 13/13 [00:04<00:00,  3.14it/s, loss=0.214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Avg Loss: 0.2164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 13/13 [00:05<00:00,  2.25it/s, loss=0.206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Avg Loss: 0.2106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 13/13 [00:04<00:00,  2.70it/s, loss=0.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Avg Loss: 0.2043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 13/13 [00:04<00:00,  3.12it/s, loss=0.192]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Avg Loss: 0.1976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 - Avg Loss: 0.1069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:25<00:00,  1.93s/it, loss=0.111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 - Avg Loss: 0.1059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:24<00:00,  1.91s/it, loss=0.105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 - Avg Loss: 0.1042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:24<00:00,  1.91s/it, loss=0.104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 - Avg Loss: 0.1032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:24<00:00,  1.90s/it, loss=0.0992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Avg Loss: 0.1018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:24<00:00,  1.91s/it, loss=0.101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 - Avg Loss: 0.1006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40:  38%|█████████████████████████████████████████▏                                                                 | 5/13 [00:11<00:18,  2.31s/it, loss=0.103]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m _, loss = vlm(imgs, input_ids, targets=target_ids)\n\u001b[32m     13\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m total_loss += loss.item()\n\u001b[32m     17\u001b[39m pbar.set_postfix({\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m: loss.item()})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/train_vlm/.env/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/train_vlm/.env/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/train_vlm/.env/lib/python3.12/site-packages/torch/optim/sgd.py:121\u001b[39m, in \u001b[36mSGD.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    118\u001b[39m grads: List[Tensor] = []\n\u001b[32m    119\u001b[39m momentum_buffer_list: List[Optional[Tensor]] = []\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m has_sparse_grad = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m sgd(\n\u001b[32m    126\u001b[39m     params,\n\u001b[32m    127\u001b[39m     grads,\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     found_inf=\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfound_inf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    140\u001b[39m )\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[33m\"\u001b[39m\u001b[33mmomentum\u001b[39m\u001b[33m\"\u001b[39m] != \u001b[32m0\u001b[39m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/train_vlm/.env/lib/python3.12/site-packages/torch/optim/sgd.py:99\u001b[39m, in \u001b[36mSGD._init_group\u001b[39m\u001b[34m(self, group, params, grads, momentum_buffer_list)\u001b[39m\n\u001b[32m     97\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[33m\"\u001b[39m\u001b[33mmomentum\u001b[39m\u001b[33m\"\u001b[39m] != \u001b[32m0\u001b[39m:\n\u001b[32m     98\u001b[39m             state = \u001b[38;5;28mself\u001b[39m.state[p]\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m             momentum_buffer_list.append(\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmomentum_buffer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m has_sparse_grad\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "vlm.train()\n",
    "for epoch in range(60):\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}\")\n",
    "    total_loss = 0\n",
    "    for imgs, input_ids, target_ids in pbar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        imgs = imgs.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        _, loss = vlm(imgs, input_ids, targets=target_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Avg Loss: {total_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1744964500753,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "Lr6G_VbLEm7L",
    "outputId": "8865130a-4299-4445-9989-55a6eea60061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(vlm.state_dict(), \"model_10.pth\")\n",
    "print(\"Model saved to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0x8bHVxvz8hB"
   },
   "source": [
    "## Eval\n",
    "### define model phải giống với lúc train, img_size=96, nếu đổi img_size phải đổi ở hàm def preprocess(example) và image_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1744964512054,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "wr6DhEADsp13",
    "outputId": "143ab83b-06f2-423a-c9e1-2529fdd8650b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjCgh2QRW5Iu"
   },
   "outputs": [],
   "source": [
    "tokenizer = spm.SentencePieceProcessor(model_file='/content/spm.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBizS78pV5yQ"
   },
   "outputs": [],
   "source": [
    "n_embed, num_hiddens, num_heads, n_layer = 128, 512, 8, 8\n",
    "image_embed_dim = num_hiddens\n",
    "img_size = 96\n",
    "patch_size = 16\n",
    "num_blocks = 2\n",
    "\n",
    "n_layer, block_size, num_hiddens = 8, 32, 512\n",
    "\n",
    "# Initialize the model\n",
    "model = VisionLanguageModel(\n",
    "    n_embed=n_embed,\n",
    "    img_embed_dim=image_embed_dim,\n",
    "    vocab_size=tokenizer.vocab_size(),\n",
    "    n_layer=n_layer,\n",
    "    img_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    num_heads=num_heads,\n",
    "    num_blocks=num_blocks,\n",
    "    emb_dropout=0.1,\n",
    "    block_dropout=0.1,\n",
    ")\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"/content/model_10.pth\", map_location=torch.device('cuda')))\n",
    "model.eval()  # set to eval mode if you're going to do inference\n",
    "\n",
    "# Load image\n",
    "img_path = '/content/image-1d100e9.jpg'  # 🔁 Replace with your actual image path\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# Preprocessing image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),  # make sure this matches ViT input\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "img_tensor = transform(image).unsqueeze(0)  # shape: [1, 3, 96, 96]\n",
    "\n",
    "# Move img_tensor to the same device as the model\n",
    "img_tensor = img_tensor.to(device)\n",
    "\n",
    "# Prepare prompt and tokenize it\n",
    "prompt = \"Question: What do respiration and combustion give out\\nChoices:\\nA. Oxygen\\nB. Carbon dioxide\\nC. Nitrogen\\nD. Heat\\nAnswer with the letter.\"\n",
    "tokens = tokenizer.encode(prompt)\n",
    "\n",
    "# Convert tokens list to a PyTorch tensor\n",
    "tokens_tensor = torch.tensor(tokens).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "# --- 4. Run inference ---\n",
    "with torch.no_grad():\n",
    "    output_tokens = model.generate(\n",
    "        img_array=img_tensor,\n",
    "        idx=tokens_tensor,  # Pass tensor instead of list\n",
    "        max_new_tokens=50\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1744965288569,
     "user": {
      "displayName": "Nhân Trần",
      "userId": "06164406065078381286"
     },
     "user_tz": -420
    },
    "id": "h9QzGJLfBnSg",
    "outputId": "91da19cb-8e3b-4f71-f934-9e701720edb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Question: What do respiration and combustion give out Choices: A. Oxygen B. Carbon dioxide C. Nitrogen D. Heat Answer with the letter.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Decode and handle special tokens ---\n",
    "# Convert tensor to list and decode\n",
    "output_tokens_list = output_tokens[0].cpu().numpy().tolist()\n",
    "\n",
    "# Remove special tokens manually (if needed)\n",
    "# For example, let's assume that 0 is the token for padding (common in many models)\n",
    "# Modify the list to remove any special tokens, if necessary\n",
    "output_tokens_list = [token for token in output_tokens_list if token != tokenizer.pad_id()]\n",
    "\n",
    "# Now, decode the remaining tokens\n",
    "output_text = tokenizer.decode(output_tokens_list)\n",
    "print(\"Answer:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JA3bwtEDspaj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00dbc84d65d14669af49353af2fe81c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "017b1fd478bc493c9eae5aafc36ca0ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e19b6faf003d4d458b22ae755644dd4e",
       "IPY_MODEL_cc0b64d960f34603b9a191b1e55eddef",
       "IPY_MODEL_50b7ca0016e74b9b86b406a58f0eb37a"
      ],
      "layout": "IPY_MODEL_42a90f9929544cbb9b3540878f1ab9ff"
     }
    },
    "0f46dd8c14094ae19910136bb2ec19a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_202fee786bf74b198c48307ede3c55c6",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2a00aa99569f4bff8d506a6a282624f1",
      "value": 100
     }
    },
    "202fee786bf74b198c48307ede3c55c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a00aa99569f4bff8d506a6a282624f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3d8c3f01e1034f129e1e06e17bad751f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3df75f9d77464f19b10483b1b04468f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f2e003ba2e445658cf925566c53afe8",
      "placeholder": "​",
      "style": "IPY_MODEL_cdc95decd0694b5396f08e64aed715ae",
      "value": " 2434/2434 [00:35&lt;00:00, 72.35 examples/s]"
     }
    },
    "42a90f9929544cbb9b3540878f1ab9ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43ae542de31241efbf99b13b7b043359": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45d13ed6fef54059ae2c0c62f2e3481d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a85684c8ea1c41288d8e4a4d4b5c6b1f",
       "IPY_MODEL_0f46dd8c14094ae19910136bb2ec19a6",
       "IPY_MODEL_aacec83f466a43779ecc05ba9fada696"
      ],
      "layout": "IPY_MODEL_e60330e7e3704f119f666acb11fb93be"
     }
    },
    "45d1881f95724550bf82bc6bc85d19fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b38adec2ad1413c8b8dabcd41a3d9ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50b7ca0016e74b9b86b406a58f0eb37a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_704f403601be451db32201221aba1b78",
      "placeholder": "​",
      "style": "IPY_MODEL_fa7f6a094ac349f2b39e59675f1d6539",
      "value": " 2434/2434 [00:34&lt;00:00, 71.16 examples/s]"
     }
    },
    "5c46284f51b7416dacae0d9a67ee986c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_637ef79ba39b4eb18061ca44840569e4",
      "max": 2434,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b31bca7a1ee94fb780cf90a8ab4bfb52",
      "value": 2434
     }
    },
    "637ef79ba39b4eb18061ca44840569e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "704f403601be451db32201221aba1b78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7aab307a4aa44c1a9f439089284a2a30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f2e003ba2e445658cf925566c53afe8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97570c5dc77d4ebf92007397ddf01d64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6ced33ea5a04e5aae45deb1db6939df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a85684c8ea1c41288d8e4a4d4b5c6b1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee9e868961f64ab5843beb0aa6841216",
      "placeholder": "​",
      "style": "IPY_MODEL_3d8c3f01e1034f129e1e06e17bad751f",
      "value": "Map: 100%"
     }
    },
    "aacec83f466a43779ecc05ba9fada696": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbac3bf0c58a417996f62f2f2dd58c4b",
      "placeholder": "​",
      "style": "IPY_MODEL_7aab307a4aa44c1a9f439089284a2a30",
      "value": " 100/100 [00:20&lt;00:00,  1.61 examples/s]"
     }
    },
    "add9d266ded843afa4819be93e994c03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b31bca7a1ee94fb780cf90a8ab4bfb52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c49155ad5e0744d3b840c444f87fe4a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45d1881f95724550bf82bc6bc85d19fd",
      "placeholder": "​",
      "style": "IPY_MODEL_97570c5dc77d4ebf92007397ddf01d64",
      "value": "Filter: 100%"
     }
    },
    "cbac3bf0c58a417996f62f2f2dd58c4b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc0b64d960f34603b9a191b1e55eddef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b38adec2ad1413c8b8dabcd41a3d9ad",
      "max": 2434,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_add9d266ded843afa4819be93e994c03",
      "value": 2434
     }
    },
    "cdc95decd0694b5396f08e64aed715ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e19b6faf003d4d458b22ae755644dd4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43ae542de31241efbf99b13b7b043359",
      "placeholder": "​",
      "style": "IPY_MODEL_a6ced33ea5a04e5aae45deb1db6939df",
      "value": "Filter: 100%"
     }
    },
    "e60330e7e3704f119f666acb11fb93be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee9e868961f64ab5843beb0aa6841216": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f03af258ff3f44b198a34a4dfffb99ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c49155ad5e0744d3b840c444f87fe4a3",
       "IPY_MODEL_5c46284f51b7416dacae0d9a67ee986c",
       "IPY_MODEL_3df75f9d77464f19b10483b1b04468f0"
      ],
      "layout": "IPY_MODEL_00dbc84d65d14669af49353af2fe81c0"
     }
    },
    "fa7f6a094ac349f2b39e59675f1d6539": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
